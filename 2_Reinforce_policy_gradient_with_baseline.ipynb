{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2_Reinforce_policy_gradient_with_baseline.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JzxfamXQHzlX",
        "colab_type": "text"
      },
      "source": [
        "# Reinforce with baseline algorithm\n",
        "\n",
        "\n",
        "\n",
        "*   Monte Carlo plays out the whole trajectory in an episode\n",
        "that is used to update the policy afterward.\n",
        "*   However, the stochastic policy may take\n",
        "different actions at the same state in different episodes\n",
        "*   This can confuse the training, since\n",
        "one sampled experience wants to increase the probability of choosing one action while\n",
        "another sampled experience may want to decrease it.\n",
        "*   We need to reduce this high variance problem in Vnilla Reinforce algo\n",
        "*   with baseline, we subtract the baseline state-value from the return, G.\n",
        "*   As a\n",
        "result, we use an advantage function A in the gradient update\n",
        "*   At = Gt - V(st) , V(s) is a value function which estimates the state value given a state.\n",
        "\n",
        "## REINFORCE with BASELINE use two neural networks, one for policy and another one for value estimation\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uOHYpK8DHY_w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Importing Libraries and cartpole env\n",
        "import torch\n",
        "import gym\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "\n",
        "\n",
        "env = gym.make('CartPole-v0')"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c4x0cdvOJk9t",
        "colab_type": "text"
      },
      "source": [
        "# PolicyNetwork Class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4P73HJxSJkXB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class PolicyNetwork():\n",
        "    def __init__(self, n_state, n_action, n_hidden=50, lr=0.001):\n",
        "\n",
        "        self.model = nn.Sequential(\n",
        "                        nn.Linear(n_state, n_hidden),\n",
        "                        nn.ReLU(),\n",
        "                        nn.Linear(n_hidden, n_action),\n",
        "                        nn.Softmax(),\n",
        "                )\n",
        "\n",
        "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr)\n",
        "\n",
        "\n",
        "    def predict(self, s):\n",
        "        \"\"\"\n",
        "        Compute the action probabilities of state s using the learning model\n",
        "        @param s: input state\n",
        "        @return: predicted policy\n",
        "        \"\"\"\n",
        "        return self.model(torch.Tensor(s))\n",
        "\n",
        "\n",
        "    def update(self, advantages, log_probs):\n",
        "        \"\"\"\n",
        "        Update the weights of the policy network given the training samples\n",
        "        @param advantages: advantage for each step in an episode\n",
        "        @param log_probs: log probability for each step\n",
        "        \"\"\"\n",
        "        policy_gradient = []\n",
        "        for log_prob, Gt in zip(log_probs, advantages):\n",
        "            policy_gradient.append(-log_prob * Gt)\n",
        "\n",
        "        loss = torch.stack(policy_gradient).sum()\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "\n",
        "    def get_action(self, s):\n",
        "        \"\"\"\n",
        "        Estimate the policy and sample an action, compute its log probability\n",
        "        @param s: input state\n",
        "        @return: the selected action and log probability\n",
        "        \"\"\"\n",
        "        probs = self.predict(s)\n",
        "        action = torch.multinomial(probs, 1).item()\n",
        "        log_prob = torch.log(probs[action])\n",
        "        return action, log_prob\n",
        "    \n",
        "\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uQucG2wHJ5Ax",
        "colab_type": "text"
      },
      "source": [
        "# Value Network\n",
        "\n",
        "## Use of Regression Neural Network with one hidden layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nqwe8o23JYwq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ValueNetwork():\n",
        "    def __init__(self, n_state, n_hidden=50, lr=0.05):\n",
        "        self.criterion = torch.nn.MSELoss() # Its learning goal is to approximate state-values; hence, we use the mean squared\n",
        "                                            # error as the loss function.\n",
        "        self.model = torch.nn.Sequential(\n",
        "                        torch.nn.Linear(n_state, n_hidden),\n",
        "                        torch.nn.ReLU(),\n",
        "                        torch.nn.Linear(n_hidden, 1)\n",
        "                )\n",
        "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr)\n",
        "\n",
        "    \"\"\"\n",
        "    The update method trains the value regression model with a set of input states\n",
        "    and target outputs, via backpropagation of course.\n",
        "    \"\"\"\n",
        "\n",
        "    def update(self, s, y):\n",
        "        \"\"\"\n",
        "        Update the weights of the DQN given a training sample\n",
        "        @param s: states\n",
        "        @param y: target values\n",
        "        \"\"\"\n",
        "        y_pred = self.model(torch.Tensor(s))\n",
        "        loss = self.criterion(y_pred, Variable(torch.Tensor(y)))\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "    # the predict method estimates the state-value:\n",
        "    def predict(self, s):\n",
        "        \"\"\"\n",
        "        Compute the Q values of the state for all actions using the learning model\n",
        "        @param s: input state\n",
        "        @return: Q values of the state for all actions\n",
        "        \"\"\"\n",
        "        with torch.no_grad():\n",
        "            return self.model(torch.Tensor(s))\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hFgmzubQKqfs",
        "colab_type": "text"
      },
      "source": [
        "# REINFORCE with BASELINE algorithm with a policy and value network model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fqs5MQfrKHJO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def reinforce(env, estimator_policy, estimator_value, n_episode, gamma=1.0):\n",
        "    \"\"\"\n",
        "    REINFORCE algorithm with baseline\n",
        "    @param env: Gym environment\n",
        "    @param estimator_policy: policy network\n",
        "    @param estimator_value: value network\n",
        "    @param n_episode: number of episodes\n",
        "    @param gamma: the discount factor\n",
        "    \"\"\"\n",
        "    for episode in range(n_episode):\n",
        "        log_probs = []\n",
        "        states = []\n",
        "        rewards = []\n",
        "        state = env.reset()\n",
        "\n",
        "        while True:\n",
        "            states.append(state)\n",
        "            action, log_prob = estimator_policy.get_action(state)\n",
        "            next_state, reward, is_done, _ = env.step(action)\n",
        "\n",
        "            total_reward_episode[episode] += reward\n",
        "            log_probs.append(log_prob)\n",
        "\n",
        "            rewards.append(reward)\n",
        "\n",
        "            if is_done:\n",
        "                Gt = 0\n",
        "                pw = 0\n",
        "\n",
        "                returns = []\n",
        "                for t in range(len(states)-1, -1, -1):\n",
        "                    Gt += gamma ** pw * rewards[t]\n",
        "                    pw += 1\n",
        "                    returns.append(Gt)\n",
        "\n",
        "\n",
        "                returns = returns[::-1]\n",
        "                returns = torch.tensor(returns)\n",
        "\n",
        "                baseline_values = estimator_value.predict(states)\n",
        "\n",
        "                advantages = returns - baseline_values\n",
        "\n",
        "\n",
        "                estimator_value.update(states, returns)\n",
        "\n",
        "                estimator_policy.update(advantages, log_probs)\n",
        "\n",
        "\n",
        "                print('Episode: {}, total reward: {}'.format(episode, total_reward_episode[episode]))\n",
        "                break\n",
        "\n",
        "\n",
        "            state = next_state\n",
        "\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZD0183siK4nZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n_state = env.observation_space.shape[0]\n",
        "n_action = env.action_space.n\n",
        "n_hidden_p = 64\n",
        "lr_p = 0.003\n",
        "policy_net = PolicyNetwork(n_state, n_action, n_hidden_p, lr_p)\n",
        "\n",
        "n_hidden_v = 64\n",
        "lr_v = 0.003\n",
        "value_net = ValueNetwork(n_state, n_hidden_v, lr_v)\n",
        "\n",
        "n_episode = 2000\n",
        "gamma = 0.9\n",
        "total_reward_episode = [0] * n_episode"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F9lCYnvGK_02",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "e6e93765-a73a-40a1-b374-cf357810eb6b"
      },
      "source": [
        "reinforce(env, policy_net, value_net, n_episode, gamma)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py:117: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  input = module(input)\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:445: UserWarning: Using a target size (torch.Size([56])) that is different to the input size (torch.Size([56, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Episode: 0, total reward: 56.0\n",
            "Episode: 1, total reward: 52.0\n",
            "Episode: 2, total reward: 67.0\n",
            "Episode: 3, total reward: 25.0\n",
            "Episode: 4, total reward: 14.0\n",
            "Episode: 5, total reward: 31.0\n",
            "Episode: 6, total reward: 15.0\n",
            "Episode: 7, total reward: 20.0\n",
            "Episode: 8, total reward: 10.0\n",
            "Episode: 9, total reward: 31.0\n",
            "Episode: 10, total reward: 33.0\n",
            "Episode: 11, total reward: 14.0\n",
            "Episode: 12, total reward: 21.0\n",
            "Episode: 13, total reward: 21.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:445: UserWarning: Using a target size (torch.Size([52])) that is different to the input size (torch.Size([52, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:445: UserWarning: Using a target size (torch.Size([67])) that is different to the input size (torch.Size([67, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:445: UserWarning: Using a target size (torch.Size([25])) that is different to the input size (torch.Size([25, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:445: UserWarning: Using a target size (torch.Size([14])) that is different to the input size (torch.Size([14, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:445: UserWarning: Using a target size (torch.Size([31])) that is different to the input size (torch.Size([31, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:445: UserWarning: Using a target size (torch.Size([15])) that is different to the input size (torch.Size([15, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:445: UserWarning: Using a target size (torch.Size([20])) that is different to the input size (torch.Size([20, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:445: UserWarning: Using a target size (torch.Size([10])) that is different to the input size (torch.Size([10, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:445: UserWarning: Using a target size (torch.Size([33])) that is different to the input size (torch.Size([33, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:445: UserWarning: Using a target size (torch.Size([21])) that is different to the input size (torch.Size([21, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:445: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:445: UserWarning: Using a target size (torch.Size([16])) that is different to the input size (torch.Size([16, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Episode: 14, total reward: 29.0\n",
            "Episode: 15, total reward: 16.0\n",
            "Episode: 16, total reward: 17.0\n",
            "Episode: 17, total reward: 15.0\n",
            "Episode: 18, total reward: 41.0\n",
            "Episode: 19, total reward: 42.0\n",
            "Episode: 20, total reward: 15.0\n",
            "Episode: 21, total reward: 19.0\n",
            "Episode: 22, total reward: 15.0\n",
            "Episode: 23, total reward: 45.0\n",
            "Episode: 24, total reward: 36.0\n",
            "Episode: 25, total reward: 33.0\n",
            "Episode: 26, total reward: 24.0\n",
            "Episode: 27, total reward: 87.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:445: UserWarning: Using a target size (torch.Size([17])) that is different to the input size (torch.Size([17, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:445: UserWarning: Using a target size (torch.Size([41])) that is different to the input size (torch.Size([41, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:445: UserWarning: Using a target size (torch.Size([42])) that is different to the input size (torch.Size([42, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:445: UserWarning: Using a target size (torch.Size([19])) that is different to the input size (torch.Size([19, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:445: UserWarning: Using a target size (torch.Size([45])) that is different to the input size (torch.Size([45, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:445: UserWarning: Using a target size (torch.Size([36])) that is different to the input size (torch.Size([36, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:445: UserWarning: Using a target size (torch.Size([24])) that is different to the input size (torch.Size([24, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:445: UserWarning: Using a target size (torch.Size([87])) that is different to the input size (torch.Size([87, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:445: UserWarning: Using a target size (torch.Size([37])) that is different to the input size (torch.Size([37, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Episode: 28, total reward: 37.0\n",
            "Episode: 29, total reward: 22.0\n",
            "Episode: 30, total reward: 36.0\n",
            "Episode: 31, total reward: 26.0\n",
            "Episode: 32, total reward: 14.0\n",
            "Episode: 33, total reward: 40.0\n",
            "Episode: 34, total reward: 33.0\n",
            "Episode: 35, total reward: 30.0\n",
            "Episode: 36, total reward: 49.0\n",
            "Episode: 37, total reward: 18.0\n",
            "Episode: 38, total reward: 29.0\n",
            "Episode: 39, total reward: 18.0\n",
            "Episode: 40, total reward: 20.0\n",
            "Episode: 41, total reward: 41.0\n",
            "Episode: 42, total reward: 11.0\n",
            "Episode: 43, total reward: 10.0\n",
            "Episode: 44, total reward: 30.0\n",
            "Episode: 45, total reward: 31.0\n",
            "Episode: 46, total reward: 49.0\n",
            "Episode: 47, total reward: 15.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:445: UserWarning: Using a target size (torch.Size([22])) that is different to the input size (torch.Size([22, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:445: UserWarning: Using a target size (torch.Size([26])) that is different to the input size (torch.Size([26, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:445: UserWarning: Using a target size (torch.Size([40])) that is different to the input size (torch.Size([40, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:445: UserWarning: Using a target size (torch.Size([30])) that is different to the input size (torch.Size([30, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:445: UserWarning: Using a target size (torch.Size([49])) that is different to the input size (torch.Size([49, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:445: UserWarning: Using a target size (torch.Size([18])) that is different to the input size (torch.Size([18, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:445: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:445: UserWarning: Using a target size (torch.Size([13])) that is different to the input size (torch.Size([13, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Episode: 48, total reward: 13.0\n",
            "Episode: 49, total reward: 19.0\n",
            "Episode: 50, total reward: 13.0\n",
            "Episode: 51, total reward: 17.0\n",
            "Episode: 52, total reward: 44.0\n",
            "Episode: 53, total reward: 20.0\n",
            "Episode: 54, total reward: 12.0\n",
            "Episode: 55, total reward: 23.0\n",
            "Episode: 56, total reward: 10.0\n",
            "Episode: 57, total reward: 63.0\n",
            "Episode: 58, total reward: 17.0\n",
            "Episode: 59, total reward: 18.0\n",
            "Episode: 60, total reward: 81.0\n",
            "Episode: 61, total reward: 34.0\n",
            "Episode: 62, total reward: 25.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:445: UserWarning: Using a target size (torch.Size([44])) that is different to the input size (torch.Size([44, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:445: UserWarning: Using a target size (torch.Size([12])) that is different to the input size (torch.Size([12, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:445: UserWarning: Using a target size (torch.Size([23])) that is different to the input size (torch.Size([23, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:445: UserWarning: Using a target size (torch.Size([63])) that is different to the input size (torch.Size([63, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:445: UserWarning: Using a target size (torch.Size([81])) that is different to the input size (torch.Size([81, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:445: UserWarning: Using a target size (torch.Size([34])) that is different to the input size (torch.Size([34, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:445: UserWarning: Using a target size (torch.Size([54])) that is different to the input size (torch.Size([54, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Episode: 63, total reward: 54.0\n",
            "Episode: 64, total reward: 30.0\n",
            "Episode: 65, total reward: 25.0\n",
            "Episode: 66, total reward: 42.0\n",
            "Episode: 67, total reward: 19.0\n",
            "Episode: 68, total reward: 27.0\n",
            "Episode: 69, total reward: 19.0\n",
            "Episode: 70, total reward: 86.0\n",
            "Episode: 71, total reward: 15.0\n",
            "Episode: 72, total reward: 26.0\n",
            "Episode: 73, total reward: 13.0\n",
            "Episode: 74, total reward: 9.0\n",
            "Episode: 75, total reward: 102.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:445: UserWarning: Using a target size (torch.Size([27])) that is different to the input size (torch.Size([27, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:445: UserWarning: Using a target size (torch.Size([86])) that is different to the input size (torch.Size([86, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:445: UserWarning: Using a target size (torch.Size([9])) that is different to the input size (torch.Size([9, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:445: UserWarning: Using a target size (torch.Size([102])) that is different to the input size (torch.Size([102, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Episode: 76, total reward: 21.0\n",
            "Episode: 77, total reward: 18.0\n",
            "Episode: 78, total reward: 29.0\n",
            "Episode: 79, total reward: 21.0\n",
            "Episode: 80, total reward: 40.0\n",
            "Episode: 81, total reward: 47.0\n",
            "Episode: 82, total reward: 36.0\n",
            "Episode: 83, total reward: 55.0\n",
            "Episode: 84, total reward: 69.0\n",
            "Episode: 85, total reward: 81.0\n",
            "Episode: 86, total reward: 17.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:445: UserWarning: Using a target size (torch.Size([47])) that is different to the input size (torch.Size([47, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:445: UserWarning: Using a target size (torch.Size([55])) that is different to the input size (torch.Size([55, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:445: UserWarning: Using a target size (torch.Size([69])) that is different to the input size (torch.Size([69, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:445: UserWarning: Using a target size (torch.Size([72])) that is different to the input size (torch.Size([72, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Episode: 87, total reward: 26.0\n",
            "Episode: 88, total reward: 27.0\n",
            "Episode: 89, total reward: 72.0\n",
            "Episode: 90, total reward: 22.0\n",
            "Episode: 91, total reward: 31.0\n",
            "Episode: 92, total reward: 11.0\n",
            "Episode: 93, total reward: 41.0\n",
            "Episode: 94, total reward: 25.0\n",
            "Episode: 95, total reward: 33.0\n",
            "Episode: 96, total reward: 18.0\n",
            "Episode: 97, total reward: 59.0\n",
            "Episode: 98, total reward: 75.0\n",
            "Episode: 99, total reward: 13.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:445: UserWarning: Using a target size (torch.Size([59])) that is different to the input size (torch.Size([59, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:445: UserWarning: Using a target size (torch.Size([75])) that is different to the input size (torch.Size([75, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:445: UserWarning: Using a target size (torch.Size([50])) that is different to the input size (torch.Size([50, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:445: UserWarning: Using a target size (torch.Size([115])) that is different to the input size (torch.Size([115, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:445: UserWarning: Using a target size (torch.Size([46])) that is different to the input size (torch.Size([46, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Episode: 100, total reward: 50.0\n",
            "Episode: 101, total reward: 115.0\n",
            "Episode: 102, total reward: 11.0\n",
            "Episode: 103, total reward: 20.0\n",
            "Episode: 104, total reward: 36.0\n",
            "Episode: 105, total reward: 16.0\n",
            "Episode: 106, total reward: 36.0\n",
            "Episode: 107, total reward: 46.0\n",
            "Episode: 108, total reward: 34.0\n",
            "Episode: 109, total reward: 25.0\n",
            "Episode: 110, total reward: 21.0\n",
            "Episode: 111, total reward: 25.0\n",
            "Episode: 112, total reward: 37.0\n",
            "Episode: 113, total reward: 35.0\n",
            "Episode: 114, total reward: 11.0\n",
            "Episode: 115, total reward: 29.0\n",
            "Episode: 116, total reward: 28.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:445: UserWarning: Using a target size (torch.Size([35])) that is different to the input size (torch.Size([35, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:445: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:445: UserWarning: Using a target size (torch.Size([79])) that is different to the input size (torch.Size([79, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:445: UserWarning: Using a target size (torch.Size([53])) that is different to the input size (torch.Size([53, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:445: UserWarning: Using a target size (torch.Size([43])) that is different to the input size (torch.Size([43, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Episode: 117, total reward: 79.0\n",
            "Episode: 118, total reward: 24.0\n",
            "Episode: 119, total reward: 53.0\n",
            "Episode: 120, total reward: 53.0\n",
            "Episode: 121, total reward: 87.0\n",
            "Episode: 122, total reward: 15.0\n",
            "Episode: 123, total reward: 43.0\n",
            "Episode: 124, total reward: 11.0\n",
            "Episode: 125, total reward: 44.0\n",
            "Episode: 126, total reward: 37.0\n",
            "Episode: 127, total reward: 65.0\n",
            "Episode: 128, total reward: 61.0\n",
            "Episode: 129, total reward: 25.0\n",
            "Episode: 130, total reward: 31.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:445: UserWarning: Using a target size (torch.Size([65])) that is different to the input size (torch.Size([65, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:445: UserWarning: Using a target size (torch.Size([61])) that is different to the input size (torch.Size([61, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:445: UserWarning: Using a target size (torch.Size([38])) that is different to the input size (torch.Size([38, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:445: UserWarning: Using a target size (torch.Size([32])) that is different to the input size (torch.Size([32, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Episode: 131, total reward: 23.0\n",
            "Episode: 132, total reward: 54.0\n",
            "Episode: 133, total reward: 29.0\n",
            "Episode: 134, total reward: 24.0\n",
            "Episode: 135, total reward: 36.0\n",
            "Episode: 136, total reward: 29.0\n",
            "Episode: 137, total reward: 38.0\n",
            "Episode: 138, total reward: 21.0\n",
            "Episode: 139, total reward: 16.0\n",
            "Episode: 140, total reward: 37.0\n",
            "Episode: 141, total reward: 32.0\n",
            "Episode: 142, total reward: 47.0\n",
            "Episode: 143, total reward: 40.0\n",
            "Episode: 144, total reward: 20.0\n",
            "Episode: 145, total reward: 33.0\n",
            "Episode: 146, total reward: 16.0\n",
            "Episode: 147, total reward: 14.0\n",
            "Episode: 148, total reward: 32.0\n",
            "Episode: 149, total reward: 27.0\n",
            "Episode: 150, total reward: 75.0\n",
            "Episode: 151, total reward: 15.0\n",
            "Episode: 152, total reward: 23.0\n",
            "Episode: 153, total reward: 34.0\n",
            "Episode: 154, total reward: 51.0\n",
            "Episode: 155, total reward: 47.0\n",
            "Episode: 156, total reward: 47.0\n",
            "Episode: 157, total reward: 46.0\n",
            "Episode: 158, total reward: 48.0\n",
            "Episode: 159, total reward: 184.0\n",
            "Episode: 160, total reward: 40.0\n",
            "Episode: 161, total reward: 48.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:445: UserWarning: Using a target size (torch.Size([51])) that is different to the input size (torch.Size([51, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:445: UserWarning: Using a target size (torch.Size([48])) that is different to the input size (torch.Size([48, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:445: UserWarning: Using a target size (torch.Size([184])) that is different to the input size (torch.Size([184, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Episode: 162, total reward: 42.0\n",
            "Episode: 163, total reward: 44.0\n",
            "Episode: 164, total reward: 19.0\n",
            "Episode: 165, total reward: 79.0\n",
            "Episode: 166, total reward: 94.0\n",
            "Episode: 167, total reward: 55.0\n",
            "Episode: 168, total reward: 84.0\n",
            "Episode: 169, total reward: 84.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:445: UserWarning: Using a target size (torch.Size([94])) that is different to the input size (torch.Size([94, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:445: UserWarning: Using a target size (torch.Size([84])) that is different to the input size (torch.Size([84, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:445: UserWarning: Using a target size (torch.Size([60])) that is different to the input size (torch.Size([60, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:445: UserWarning: Using a target size (torch.Size([58])) that is different to the input size (torch.Size([58, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Episode: 170, total reward: 60.0\n",
            "Episode: 171, total reward: 25.0\n",
            "Episode: 172, total reward: 61.0\n",
            "Episode: 173, total reward: 36.0\n",
            "Episode: 174, total reward: 58.0\n",
            "Episode: 175, total reward: 19.0\n",
            "Episode: 176, total reward: 43.0\n",
            "Episode: 177, total reward: 37.0\n",
            "Episode: 178, total reward: 29.0\n",
            "Episode: 179, total reward: 63.0\n",
            "Episode: 180, total reward: 60.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:445: UserWarning: Using a target size (torch.Size([151])) that is different to the input size (torch.Size([151, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:445: UserWarning: Using a target size (torch.Size([147])) that is different to the input size (torch.Size([147, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:445: UserWarning: Using a target size (torch.Size([117])) that is different to the input size (torch.Size([117, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:445: UserWarning: Using a target size (torch.Size([76])) that is different to the input size (torch.Size([76, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Episode: 181, total reward: 151.0\n",
            "Episode: 182, total reward: 44.0\n",
            "Episode: 183, total reward: 147.0\n",
            "Episode: 184, total reward: 56.0\n",
            "Episode: 185, total reward: 117.0\n",
            "Episode: 186, total reward: 76.0\n",
            "Episode: 187, total reward: 67.0\n",
            "Episode: 188, total reward: 49.0\n",
            "Episode: 189, total reward: 65.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:445: UserWarning: Using a target size (torch.Size([91])) that is different to the input size (torch.Size([91, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:445: UserWarning: Using a target size (torch.Size([154])) that is different to the input size (torch.Size([154, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Episode: 190, total reward: 91.0\n",
            "Episode: 191, total reward: 25.0\n",
            "Episode: 192, total reward: 45.0\n",
            "Episode: 193, total reward: 30.0\n",
            "Episode: 194, total reward: 154.0\n",
            "Episode: 195, total reward: 50.0\n",
            "Episode: 196, total reward: 36.0\n",
            "Episode: 197, total reward: 42.0\n",
            "Episode: 198, total reward: 27.0\n",
            "Episode: 199, total reward: 42.0\n",
            "Episode: 200, total reward: 58.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:445: UserWarning: Using a target size (torch.Size([130])) that is different to the input size (torch.Size([130, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:445: UserWarning: Using a target size (torch.Size([114])) that is different to the input size (torch.Size([114, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:445: UserWarning: Using a target size (torch.Size([88])) that is different to the input size (torch.Size([88, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:445: UserWarning: Using a target size (torch.Size([90])) that is different to the input size (torch.Size([90, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:445: UserWarning: Using a target size (torch.Size([200])) that is different to the input size (torch.Size([200, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Episode: 201, total reward: 130.0\n",
            "Episode: 202, total reward: 114.0\n",
            "Episode: 203, total reward: 56.0\n",
            "Episode: 204, total reward: 88.0\n",
            "Episode: 205, total reward: 94.0\n",
            "Episode: 206, total reward: 90.0\n",
            "Episode: 207, total reward: 200.0\n",
            "Episode: 208, total reward: 40.0\n",
            "Episode: 209, total reward: 47.0\n",
            "Episode: 210, total reward: 109.0\n",
            "Episode: 211, total reward: 55.0\n",
            "Episode: 212, total reward: 105.0\n",
            "Episode: 213, total reward: 34.0\n",
            "Episode: 214, total reward: 100.0\n",
            "Episode: 215, total reward: 97.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:445: UserWarning: Using a target size (torch.Size([109])) that is different to the input size (torch.Size([109, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:445: UserWarning: Using a target size (torch.Size([105])) that is different to the input size (torch.Size([105, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:445: UserWarning: Using a target size (torch.Size([100])) that is different to the input size (torch.Size([100, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:445: UserWarning: Using a target size (torch.Size([97])) that is different to the input size (torch.Size([97, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Episode: 216, total reward: 27.0\n",
            "Episode: 217, total reward: 48.0\n",
            "Episode: 218, total reward: 37.0\n",
            "Episode: 219, total reward: 86.0\n",
            "Episode: 220, total reward: 94.0\n",
            "Episode: 221, total reward: 76.0\n",
            "Episode: 222, total reward: 95.0\n",
            "Episode: 223, total reward: 91.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:445: UserWarning: Using a target size (torch.Size([95])) that is different to the input size (torch.Size([95, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:445: UserWarning: Using a target size (torch.Size([98])) that is different to the input size (torch.Size([98, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:445: UserWarning: Using a target size (torch.Size([108])) that is different to the input size (torch.Size([108, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:445: UserWarning: Using a target size (torch.Size([140])) that is different to the input size (torch.Size([140, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Episode: 224, total reward: 98.0\n",
            "Episode: 225, total reward: 81.0\n",
            "Episode: 226, total reward: 108.0\n",
            "Episode: 227, total reward: 140.0\n",
            "Episode: 228, total reward: 76.0\n",
            "Episode: 229, total reward: 48.0\n",
            "Episode: 230, total reward: 26.0\n",
            "Episode: 231, total reward: 96.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:445: UserWarning: Using a target size (torch.Size([96])) that is different to the input size (torch.Size([96, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:445: UserWarning: Using a target size (torch.Size([120])) that is different to the input size (torch.Size([120, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:445: UserWarning: Using a target size (torch.Size([68])) that is different to the input size (torch.Size([68, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:445: UserWarning: Using a target size (torch.Size([62])) that is different to the input size (torch.Size([62, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Episode: 232, total reward: 48.0\n",
            "Episode: 233, total reward: 27.0\n",
            "Episode: 234, total reward: 120.0\n",
            "Episode: 235, total reward: 68.0\n",
            "Episode: 236, total reward: 98.0\n",
            "Episode: 237, total reward: 91.0\n",
            "Episode: 238, total reward: 62.0\n",
            "Episode: 239, total reward: 109.0\n",
            "Episode: 240, total reward: 55.0\n",
            "Episode: 241, total reward: 142.0\n",
            "Episode: 242, total reward: 80.0\n",
            "Episode: 243, total reward: 75.0\n",
            "Episode: 244, total reward: 102.0\n",
            "Episode: 245, total reward: 155.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:445: UserWarning: Using a target size (torch.Size([142])) that is different to the input size (torch.Size([142, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:445: UserWarning: Using a target size (torch.Size([80])) that is different to the input size (torch.Size([80, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:445: UserWarning: Using a target size (torch.Size([155])) that is different to the input size (torch.Size([155, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:445: UserWarning: Using a target size (torch.Size([180])) that is different to the input size (torch.Size([180, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Episode: 246, total reward: 180.0\n",
            "Episode: 247, total reward: 169.0\n",
            "Episode: 248, total reward: 159.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:445: UserWarning: Using a target size (torch.Size([169])) that is different to the input size (torch.Size([169, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:445: UserWarning: Using a target size (torch.Size([159])) that is different to the input size (torch.Size([159, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:445: UserWarning: Using a target size (torch.Size([195])) that is different to the input size (torch.Size([195, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Episode: 249, total reward: 195.0\n",
            "Episode: 250, total reward: 120.0\n",
            "Episode: 251, total reward: 189.0\n",
            "Episode: 252, total reward: 200.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:445: UserWarning: Using a target size (torch.Size([189])) that is different to the input size (torch.Size([189, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:445: UserWarning: Using a target size (torch.Size([57])) that is different to the input size (torch.Size([57, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Episode: 253, total reward: 130.0\n",
            "Episode: 254, total reward: 91.0\n",
            "Episode: 255, total reward: 57.0\n",
            "Episode: 256, total reward: 68.0\n",
            "Episode: 257, total reward: 140.0\n",
            "Episode: 258, total reward: 108.0\n",
            "Episode: 259, total reward: 55.0\n",
            "Episode: 260, total reward: 95.0\n",
            "Episode: 261, total reward: 88.0\n",
            "Episode: 262, total reward: 92.0\n",
            "Episode: 263, total reward: 157.0\n",
            "Episode: 264, total reward: 64.0\n",
            "Episode: 265, total reward: 155.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:445: UserWarning: Using a target size (torch.Size([92])) that is different to the input size (torch.Size([92, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:445: UserWarning: Using a target size (torch.Size([157])) that is different to the input size (torch.Size([157, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:445: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Episode: 266, total reward: 169.0\n",
            "Episode: 267, total reward: 168.0\n",
            "Episode: 268, total reward: 68.0\n",
            "Episode: 269, total reward: 132.0\n",
            "Episode: 270, total reward: 200.0\n",
            "Episode: 271, total reward: 31.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:445: UserWarning: Using a target size (torch.Size([168])) that is different to the input size (torch.Size([168, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:445: UserWarning: Using a target size (torch.Size([132])) that is different to the input size (torch.Size([132, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Episode: 272, total reward: 140.0\n",
            "Episode: 273, total reward: 200.0\n",
            "Episode: 274, total reward: 200.0\n",
            "Episode: 275, total reward: 200.0\n",
            "Episode: 276, total reward: 200.0\n",
            "Episode: 277, total reward: 99.0\n",
            "Episode: 278, total reward: 185.0\n",
            "Episode: 279, total reward: 167.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:445: UserWarning: Using a target size (torch.Size([99])) that is different to the input size (torch.Size([99, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:445: UserWarning: Using a target size (torch.Size([185])) that is different to the input size (torch.Size([185, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:445: UserWarning: Using a target size (torch.Size([167])) that is different to the input size (torch.Size([167, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Episode: 280, total reward: 200.0\n",
            "Episode: 281, total reward: 167.0\n",
            "Episode: 282, total reward: 200.0\n",
            "Episode: 283, total reward: 197.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:445: UserWarning: Using a target size (torch.Size([197])) that is different to the input size (torch.Size([197, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:445: UserWarning: Using a target size (torch.Size([139])) that is different to the input size (torch.Size([139, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Episode: 284, total reward: 142.0\n",
            "Episode: 285, total reward: 200.0\n",
            "Episode: 286, total reward: 139.0\n",
            "Episode: 287, total reward: 200.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:445: UserWarning: Using a target size (torch.Size([182])) that is different to the input size (torch.Size([182, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:445: UserWarning: Using a target size (torch.Size([66])) that is different to the input size (torch.Size([66, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Episode: 288, total reward: 182.0\n",
            "Episode: 289, total reward: 200.0\n",
            "Episode: 290, total reward: 66.0\n",
            "Episode: 291, total reward: 76.0\n",
            "Episode: 292, total reward: 200.0\n",
            "Episode: 293, total reward: 182.0\n",
            "Episode: 294, total reward: 94.0\n",
            "Episode: 295, total reward: 200.0\n",
            "Episode: 296, total reward: 200.0\n",
            "Episode: 297, total reward: 200.0\n",
            "Episode: 298, total reward: 200.0\n",
            "Episode: 299, total reward: 150.0\n",
            "Episode: 300, total reward: 180.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:445: UserWarning: Using a target size (torch.Size([150])) that is different to the input size (torch.Size([150, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Episode: 301, total reward: 200.0\n",
            "Episode: 302, total reward: 158.0\n",
            "Episode: 303, total reward: 200.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:445: UserWarning: Using a target size (torch.Size([158])) that is different to the input size (torch.Size([158, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Episode: 304, total reward: 200.0\n",
            "Episode: 305, total reward: 200.0\n",
            "Episode: 306, total reward: 200.0\n",
            "Episode: 307, total reward: 31.0\n",
            "Episode: 308, total reward: 200.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:445: UserWarning: Using a target size (torch.Size([181])) that is different to the input size (torch.Size([181, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:445: UserWarning: Using a target size (torch.Size([190])) that is different to the input size (torch.Size([190, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:445: UserWarning: Using a target size (torch.Size([179])) that is different to the input size (torch.Size([179, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Episode: 309, total reward: 181.0\n",
            "Episode: 310, total reward: 200.0\n",
            "Episode: 311, total reward: 190.0\n",
            "Episode: 312, total reward: 179.0\n",
            "Episode: 313, total reward: 200.0\n",
            "Episode: 314, total reward: 118.0\n",
            "Episode: 315, total reward: 200.0\n",
            "Episode: 316, total reward: 200.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:445: UserWarning: Using a target size (torch.Size([118])) that is different to the input size (torch.Size([118, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Episode: 317, total reward: 200.0\n",
            "Episode: 318, total reward: 200.0\n",
            "Episode: 319, total reward: 197.0\n",
            "Episode: 320, total reward: 200.0\n",
            "Episode: 321, total reward: 200.0\n",
            "Episode: 322, total reward: 184.0\n",
            "Episode: 323, total reward: 172.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:445: UserWarning: Using a target size (torch.Size([172])) that is different to the input size (torch.Size([172, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:445: UserWarning: Using a target size (torch.Size([149])) that is different to the input size (torch.Size([149, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Episode: 324, total reward: 154.0\n",
            "Episode: 325, total reward: 149.0\n",
            "Episode: 326, total reward: 200.0\n",
            "Episode: 327, total reward: 200.0\n",
            "Episode: 328, total reward: 200.0\n",
            "Episode: 329, total reward: 63.0\n",
            "Episode: 330, total reward: 200.0\n",
            "Episode: 331, total reward: 74.0\n",
            "Episode: 332, total reward: 200.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:445: UserWarning: Using a target size (torch.Size([74])) that is different to the input size (torch.Size([74, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Episode: 333, total reward: 200.0\n",
            "Episode: 334, total reward: 200.0\n",
            "Episode: 335, total reward: 156.0\n",
            "Episode: 336, total reward: 156.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:445: UserWarning: Using a target size (torch.Size([156])) that is different to the input size (torch.Size([156, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:445: UserWarning: Using a target size (torch.Size([127])) that is different to the input size (torch.Size([127, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Episode: 337, total reward: 200.0\n",
            "Episode: 338, total reward: 127.0\n",
            "Episode: 339, total reward: 200.0\n",
            "Episode: 340, total reward: 200.0\n",
            "Episode: 341, total reward: 108.0\n",
            "Episode: 342, total reward: 200.0\n",
            "Episode: 343, total reward: 200.0\n",
            "Episode: 344, total reward: 187.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:445: UserWarning: Using a target size (torch.Size([187])) that is different to the input size (torch.Size([187, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:445: UserWarning: Using a target size (torch.Size([193])) that is different to the input size (torch.Size([193, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Episode: 345, total reward: 200.0\n",
            "Episode: 346, total reward: 193.0\n",
            "Episode: 347, total reward: 200.0\n",
            "Episode: 348, total reward: 172.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:445: UserWarning: Using a target size (torch.Size([123])) that is different to the input size (torch.Size([123, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:445: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Episode: 349, total reward: 123.0\n",
            "Episode: 350, total reward: 200.0\n",
            "Episode: 351, total reward: 200.0\n",
            "Episode: 352, total reward: 76.0\n",
            "Episode: 353, total reward: 128.0\n",
            "Episode: 354, total reward: 200.0\n",
            "Episode: 355, total reward: 200.0\n",
            "Episode: 356, total reward: 179.0\n",
            "Episode: 357, total reward: 181.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:445: UserWarning: Using a target size (torch.Size([135])) that is different to the input size (torch.Size([135, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Episode: 358, total reward: 135.0\n",
            "Episode: 359, total reward: 200.0\n",
            "Episode: 360, total reward: 200.0\n",
            "Episode: 361, total reward: 200.0\n",
            "Episode: 362, total reward: 200.0\n",
            "Episode: 363, total reward: 200.0\n",
            "Episode: 364, total reward: 200.0\n",
            "Episode: 365, total reward: 200.0\n",
            "Episode: 366, total reward: 200.0\n",
            "Episode: 367, total reward: 128.0\n",
            "Episode: 368, total reward: 200.0\n",
            "Episode: 369, total reward: 200.0\n",
            "Episode: 370, total reward: 200.0\n",
            "Episode: 371, total reward: 200.0\n",
            "Episode: 372, total reward: 200.0\n",
            "Episode: 373, total reward: 200.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:445: UserWarning: Using a target size (torch.Size([174])) that is different to the input size (torch.Size([174, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:445: UserWarning: Using a target size (torch.Size([138])) that is different to the input size (torch.Size([138, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Episode: 374, total reward: 174.0\n",
            "Episode: 375, total reward: 200.0\n",
            "Episode: 376, total reward: 200.0\n",
            "Episode: 377, total reward: 138.0\n",
            "Episode: 378, total reward: 200.0\n",
            "Episode: 379, total reward: 200.0\n",
            "Episode: 380, total reward: 200.0\n",
            "Episode: 381, total reward: 200.0\n",
            "Episode: 382, total reward: 200.0\n",
            "Episode: 383, total reward: 200.0\n",
            "Episode: 384, total reward: 200.0\n",
            "Episode: 385, total reward: 200.0\n",
            "Episode: 386, total reward: 200.0\n",
            "Episode: 387, total reward: 200.0\n",
            "Episode: 388, total reward: 200.0\n",
            "Episode: 389, total reward: 200.0\n",
            "Episode: 390, total reward: 200.0\n",
            "Episode: 391, total reward: 200.0\n",
            "Episode: 392, total reward: 200.0\n",
            "Episode: 393, total reward: 200.0\n",
            "Episode: 394, total reward: 200.0\n",
            "Episode: 395, total reward: 200.0\n",
            "Episode: 396, total reward: 200.0\n",
            "Episode: 397, total reward: 200.0\n",
            "Episode: 398, total reward: 200.0\n",
            "Episode: 399, total reward: 200.0\n",
            "Episode: 400, total reward: 200.0\n",
            "Episode: 401, total reward: 200.0\n",
            "Episode: 402, total reward: 200.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:445: UserWarning: Using a target size (torch.Size([162])) that is different to the input size (torch.Size([162, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:445: UserWarning: Using a target size (torch.Size([177])) that is different to the input size (torch.Size([177, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Episode: 403, total reward: 162.0\n",
            "Episode: 404, total reward: 200.0\n",
            "Episode: 405, total reward: 177.0\n",
            "Episode: 406, total reward: 200.0\n",
            "Episode: 407, total reward: 200.0\n",
            "Episode: 408, total reward: 200.0\n",
            "Episode: 409, total reward: 200.0\n",
            "Episode: 410, total reward: 200.0\n",
            "Episode: 411, total reward: 200.0\n",
            "Episode: 412, total reward: 200.0\n",
            "Episode: 413, total reward: 200.0\n",
            "Episode: 414, total reward: 200.0\n",
            "Episode: 415, total reward: 134.0\n",
            "Episode: 416, total reward: 200.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:445: UserWarning: Using a target size (torch.Size([134])) that is different to the input size (torch.Size([134, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Episode: 417, total reward: 200.0\n",
            "Episode: 418, total reward: 200.0\n",
            "Episode: 419, total reward: 123.0\n",
            "Episode: 420, total reward: 200.0\n",
            "Episode: 421, total reward: 200.0\n",
            "Episode: 422, total reward: 200.0\n",
            "Episode: 423, total reward: 200.0\n",
            "Episode: 424, total reward: 200.0\n",
            "Episode: 425, total reward: 200.0\n",
            "Episode: 426, total reward: 200.0\n",
            "Episode: 427, total reward: 200.0\n",
            "Episode: 428, total reward: 200.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:445: UserWarning: Using a target size (torch.Size([191])) that is different to the input size (torch.Size([191, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:445: UserWarning: Using a target size (torch.Size([83])) that is different to the input size (torch.Size([83, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Episode: 429, total reward: 191.0\n",
            "Episode: 430, total reward: 200.0\n",
            "Episode: 431, total reward: 83.0\n",
            "Episode: 432, total reward: 200.0\n",
            "Episode: 433, total reward: 200.0\n",
            "Episode: 434, total reward: 200.0\n",
            "Episode: 435, total reward: 200.0\n",
            "Episode: 436, total reward: 200.0\n",
            "Episode: 437, total reward: 200.0\n",
            "Episode: 438, total reward: 200.0\n",
            "Episode: 439, total reward: 193.0\n",
            "Episode: 440, total reward: 134.0\n",
            "Episode: 441, total reward: 200.0\n",
            "Episode: 442, total reward: 200.0\n",
            "Episode: 443, total reward: 200.0\n",
            "Episode: 444, total reward: 200.0\n",
            "Episode: 445, total reward: 200.0\n",
            "Episode: 446, total reward: 200.0\n",
            "Episode: 447, total reward: 200.0\n",
            "Episode: 448, total reward: 200.0\n",
            "Episode: 449, total reward: 200.0\n",
            "Episode: 450, total reward: 194.0\n",
            "Episode: 451, total reward: 200.0\n",
            "Episode: 452, total reward: 200.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:445: UserWarning: Using a target size (torch.Size([194])) that is different to the input size (torch.Size([194, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Episode: 453, total reward: 200.0\n",
            "Episode: 454, total reward: 200.0\n",
            "Episode: 455, total reward: 200.0\n",
            "Episode: 456, total reward: 200.0\n",
            "Episode: 457, total reward: 200.0\n",
            "Episode: 458, total reward: 200.0\n",
            "Episode: 459, total reward: 200.0\n",
            "Episode: 460, total reward: 200.0\n",
            "Episode: 461, total reward: 200.0\n",
            "Episode: 462, total reward: 200.0\n",
            "Episode: 463, total reward: 200.0\n",
            "Episode: 464, total reward: 200.0\n",
            "Episode: 465, total reward: 200.0\n",
            "Episode: 466, total reward: 200.0\n",
            "Episode: 467, total reward: 200.0\n",
            "Episode: 468, total reward: 200.0\n",
            "Episode: 469, total reward: 200.0\n",
            "Episode: 470, total reward: 148.0\n",
            "Episode: 471, total reward: 200.0\n",
            "Episode: 472, total reward: 149.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:445: UserWarning: Using a target size (torch.Size([148])) that is different to the input size (torch.Size([148, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Episode: 473, total reward: 197.0\n",
            "Episode: 474, total reward: 200.0\n",
            "Episode: 475, total reward: 200.0\n",
            "Episode: 476, total reward: 198.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:445: UserWarning: Using a target size (torch.Size([198])) that is different to the input size (torch.Size([198, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Episode: 477, total reward: 200.0\n",
            "Episode: 478, total reward: 200.0\n",
            "Episode: 479, total reward: 123.0\n",
            "Episode: 480, total reward: 194.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:445: UserWarning: Using a target size (torch.Size([161])) that is different to the input size (torch.Size([161, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Episode: 481, total reward: 161.0\n",
            "Episode: 482, total reward: 200.0\n",
            "Episode: 483, total reward: 190.0\n",
            "Episode: 484, total reward: 177.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:445: UserWarning: Using a target size (torch.Size([163])) that is different to the input size (torch.Size([163, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:445: UserWarning: Using a target size (torch.Size([160])) that is different to the input size (torch.Size([160, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Episode: 485, total reward: 163.0\n",
            "Episode: 486, total reward: 200.0\n",
            "Episode: 487, total reward: 157.0\n",
            "Episode: 488, total reward: 160.0\n",
            "Episode: 489, total reward: 200.0\n",
            "Episode: 490, total reward: 188.0\n",
            "Episode: 491, total reward: 170.0\n",
            "Episode: 492, total reward: 159.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:445: UserWarning: Using a target size (torch.Size([188])) that is different to the input size (torch.Size([188, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:445: UserWarning: Using a target size (torch.Size([170])) that is different to the input size (torch.Size([170, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Episode: 493, total reward: 200.0\n",
            "Episode: 494, total reward: 200.0\n",
            "Episode: 495, total reward: 163.0\n",
            "Episode: 496, total reward: 200.0\n",
            "Episode: 497, total reward: 200.0\n",
            "Episode: 498, total reward: 166.0\n",
            "Episode: 499, total reward: 199.0\n",
            "Episode: 500, total reward: 144.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:445: UserWarning: Using a target size (torch.Size([166])) that is different to the input size (torch.Size([166, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:445: UserWarning: Using a target size (torch.Size([199])) that is different to the input size (torch.Size([199, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:445: UserWarning: Using a target size (torch.Size([144])) that is different to the input size (torch.Size([144, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Episode: 501, total reward: 200.0\n",
            "Episode: 502, total reward: 162.0\n",
            "Episode: 503, total reward: 186.0\n",
            "Episode: 504, total reward: 197.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:445: UserWarning: Using a target size (torch.Size([186])) that is different to the input size (torch.Size([186, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:445: UserWarning: Using a target size (torch.Size([175])) that is different to the input size (torch.Size([175, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Episode: 505, total reward: 175.0\n",
            "Episode: 506, total reward: 175.0\n",
            "Episode: 507, total reward: 199.0\n",
            "Episode: 508, total reward: 200.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:445: UserWarning: Using a target size (torch.Size([171])) that is different to the input size (torch.Size([171, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Episode: 509, total reward: 171.0\n",
            "Episode: 510, total reward: 200.0\n",
            "Episode: 511, total reward: 200.0\n",
            "Episode: 512, total reward: 200.0\n",
            "Episode: 513, total reward: 127.0\n",
            "Episode: 514, total reward: 200.0\n",
            "Episode: 515, total reward: 136.0\n",
            "Episode: 516, total reward: 200.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:445: UserWarning: Using a target size (torch.Size([136])) that is different to the input size (torch.Size([136, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Episode: 517, total reward: 200.0\n",
            "Episode: 518, total reward: 200.0\n",
            "Episode: 519, total reward: 200.0\n",
            "Episode: 520, total reward: 200.0\n",
            "Episode: 521, total reward: 200.0\n",
            "Episode: 522, total reward: 200.0\n",
            "Episode: 523, total reward: 200.0\n",
            "Episode: 524, total reward: 200.0\n",
            "Episode: 525, total reward: 200.0\n",
            "Episode: 526, total reward: 200.0\n",
            "Episode: 527, total reward: 200.0\n",
            "Episode: 528, total reward: 200.0\n",
            "Episode: 529, total reward: 200.0\n",
            "Episode: 530, total reward: 200.0\n",
            "Episode: 531, total reward: 200.0\n",
            "Episode: 532, total reward: 200.0\n",
            "Episode: 533, total reward: 200.0\n",
            "Episode: 534, total reward: 200.0\n",
            "Episode: 535, total reward: 200.0\n",
            "Episode: 536, total reward: 200.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:445: UserWarning: Using a target size (torch.Size([178])) that is different to the input size (torch.Size([178, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Episode: 537, total reward: 178.0\n",
            "Episode: 538, total reward: 200.0\n",
            "Episode: 539, total reward: 200.0\n",
            "Episode: 540, total reward: 200.0\n",
            "Episode: 541, total reward: 200.0\n",
            "Episode: 542, total reward: 200.0\n",
            "Episode: 543, total reward: 200.0\n",
            "Episode: 544, total reward: 200.0\n",
            "Episode: 545, total reward: 200.0\n",
            "Episode: 546, total reward: 200.0\n",
            "Episode: 547, total reward: 113.0\n",
            "Episode: 548, total reward: 200.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:445: UserWarning: Using a target size (torch.Size([113])) that is different to the input size (torch.Size([113, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Episode: 549, total reward: 200.0\n",
            "Episode: 550, total reward: 118.0\n",
            "Episode: 551, total reward: 200.0\n",
            "Episode: 552, total reward: 154.0\n",
            "Episode: 553, total reward: 182.0\n",
            "Episode: 554, total reward: 200.0\n",
            "Episode: 555, total reward: 200.0\n",
            "Episode: 556, total reward: 200.0\n",
            "Episode: 557, total reward: 192.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:445: UserWarning: Using a target size (torch.Size([192])) that is different to the input size (torch.Size([192, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Episode: 558, total reward: 177.0\n",
            "Episode: 559, total reward: 30.0\n",
            "Episode: 560, total reward: 200.0\n",
            "Episode: 561, total reward: 200.0\n",
            "Episode: 562, total reward: 135.0\n",
            "Episode: 563, total reward: 200.0\n",
            "Episode: 564, total reward: 199.0\n",
            "Episode: 565, total reward: 200.0\n",
            "Episode: 566, total reward: 200.0\n",
            "Episode: 567, total reward: 184.0\n",
            "Episode: 568, total reward: 199.0\n",
            "Episode: 569, total reward: 193.0\n",
            "Episode: 570, total reward: 191.0\n",
            "Episode: 571, total reward: 200.0\n",
            "Episode: 572, total reward: 174.0\n",
            "Episode: 573, total reward: 178.0\n",
            "Episode: 574, total reward: 200.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:445: UserWarning: Using a target size (torch.Size([176])) that is different to the input size (torch.Size([176, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Episode: 575, total reward: 176.0\n",
            "Episode: 576, total reward: 200.0\n",
            "Episode: 577, total reward: 140.0\n",
            "Episode: 578, total reward: 200.0\n",
            "Episode: 579, total reward: 200.0\n",
            "Episode: 580, total reward: 164.0\n",
            "Episode: 581, total reward: 182.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:445: UserWarning: Using a target size (torch.Size([164])) that is different to the input size (torch.Size([164, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Episode: 582, total reward: 134.0\n",
            "Episode: 583, total reward: 191.0\n",
            "Episode: 584, total reward: 200.0\n",
            "Episode: 585, total reward: 198.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:445: UserWarning: Using a target size (torch.Size([173])) that is different to the input size (torch.Size([173, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:445: UserWarning: Using a target size (torch.Size([133])) that is different to the input size (torch.Size([133, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:445: UserWarning: Using a target size (torch.Size([146])) that is different to the input size (torch.Size([146, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:445: UserWarning: Using a target size (torch.Size([141])) that is different to the input size (torch.Size([141, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Episode: 586, total reward: 173.0\n",
            "Episode: 587, total reward: 174.0\n",
            "Episode: 588, total reward: 133.0\n",
            "Episode: 589, total reward: 146.0\n",
            "Episode: 590, total reward: 141.0\n",
            "Episode: 591, total reward: 200.0\n",
            "Episode: 592, total reward: 152.0\n",
            "Episode: 593, total reward: 200.0\n",
            "Episode: 594, total reward: 182.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:445: UserWarning: Using a target size (torch.Size([152])) that is different to the input size (torch.Size([152, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Episode: 595, total reward: 163.0\n",
            "Episode: 596, total reward: 130.0\n",
            "Episode: 597, total reward: 21.0\n",
            "Episode: 598, total reward: 182.0\n",
            "Episode: 599, total reward: 73.0\n",
            "Episode: 600, total reward: 200.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:445: UserWarning: Using a target size (torch.Size([73])) that is different to the input size (torch.Size([73, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Episode: 601, total reward: 200.0\n",
            "Episode: 602, total reward: 188.0\n",
            "Episode: 603, total reward: 170.0\n",
            "Episode: 604, total reward: 158.0\n",
            "Episode: 605, total reward: 200.0\n",
            "Episode: 606, total reward: 200.0\n",
            "Episode: 607, total reward: 200.0\n",
            "Episode: 608, total reward: 46.0\n",
            "Episode: 609, total reward: 200.0\n",
            "Episode: 610, total reward: 200.0\n",
            "Episode: 611, total reward: 200.0\n",
            "Episode: 612, total reward: 200.0\n",
            "Episode: 613, total reward: 200.0\n",
            "Episode: 614, total reward: 200.0\n",
            "Episode: 615, total reward: 178.0\n",
            "Episode: 616, total reward: 200.0\n",
            "Episode: 617, total reward: 174.0\n",
            "Episode: 618, total reward: 191.0\n",
            "Episode: 619, total reward: 166.0\n",
            "Episode: 620, total reward: 194.0\n",
            "Episode: 621, total reward: 165.0\n",
            "Episode: 622, total reward: 140.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:445: UserWarning: Using a target size (torch.Size([165])) that is different to the input size (torch.Size([165, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Episode: 623, total reward: 162.0\n",
            "Episode: 624, total reward: 40.0\n",
            "Episode: 625, total reward: 163.0\n",
            "Episode: 626, total reward: 160.0\n",
            "Episode: 627, total reward: 167.0\n",
            "Episode: 628, total reward: 128.0\n",
            "Episode: 629, total reward: 193.0\n",
            "Episode: 630, total reward: 120.0\n",
            "Episode: 631, total reward: 96.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:445: UserWarning: Using a target size (torch.Size([153])) that is different to the input size (torch.Size([153, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:445: UserWarning: Using a target size (torch.Size([121])) that is different to the input size (torch.Size([121, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:445: UserWarning: Using a target size (torch.Size([124])) that is different to the input size (torch.Size([124, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Episode: 632, total reward: 153.0\n",
            "Episode: 633, total reward: 121.0\n",
            "Episode: 634, total reward: 142.0\n",
            "Episode: 635, total reward: 124.0\n",
            "Episode: 636, total reward: 109.0\n",
            "Episode: 637, total reward: 121.0\n",
            "Episode: 638, total reward: 152.0\n",
            "Episode: 639, total reward: 110.0\n",
            "Episode: 640, total reward: 162.0\n",
            "Episode: 641, total reward: 148.0\n",
            "Episode: 642, total reward: 87.0\n",
            "Episode: 643, total reward: 95.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:445: UserWarning: Using a target size (torch.Size([110])) that is different to the input size (torch.Size([110, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Episode: 644, total reward: 158.0\n",
            "Episode: 645, total reward: 97.0\n",
            "Episode: 646, total reward: 139.0\n",
            "Episode: 647, total reward: 178.0\n",
            "Episode: 648, total reward: 141.0\n",
            "Episode: 649, total reward: 200.0\n",
            "Episode: 650, total reward: 171.0\n",
            "Episode: 651, total reward: 145.0\n",
            "Episode: 652, total reward: 185.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:445: UserWarning: Using a target size (torch.Size([145])) that is different to the input size (torch.Size([145, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Episode: 653, total reward: 200.0\n",
            "Episode: 654, total reward: 200.0\n",
            "Episode: 655, total reward: 135.0\n",
            "Episode: 656, total reward: 156.0\n",
            "Episode: 657, total reward: 200.0\n",
            "Episode: 658, total reward: 137.0\n",
            "Episode: 659, total reward: 162.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:445: UserWarning: Using a target size (torch.Size([137])) that is different to the input size (torch.Size([137, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Episode: 660, total reward: 200.0\n",
            "Episode: 661, total reward: 183.0\n",
            "Episode: 662, total reward: 200.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:445: UserWarning: Using a target size (torch.Size([183])) that is different to the input size (torch.Size([183, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Episode: 663, total reward: 161.0\n",
            "Episode: 664, total reward: 162.0\n",
            "Episode: 665, total reward: 186.0\n",
            "Episode: 666, total reward: 133.0\n",
            "Episode: 667, total reward: 150.0\n",
            "Episode: 668, total reward: 167.0\n",
            "Episode: 669, total reward: 137.0\n",
            "Episode: 670, total reward: 132.0\n",
            "Episode: 671, total reward: 161.0\n",
            "Episode: 672, total reward: 166.0\n",
            "Episode: 673, total reward: 174.0\n",
            "Episode: 674, total reward: 200.0\n",
            "Episode: 675, total reward: 200.0\n",
            "Episode: 676, total reward: 200.0\n",
            "Episode: 677, total reward: 200.0\n",
            "Episode: 678, total reward: 200.0\n",
            "Episode: 679, total reward: 161.0\n",
            "Episode: 680, total reward: 200.0\n",
            "Episode: 681, total reward: 200.0\n",
            "Episode: 682, total reward: 200.0\n",
            "Episode: 683, total reward: 200.0\n",
            "Episode: 684, total reward: 200.0\n",
            "Episode: 685, total reward: 200.0\n",
            "Episode: 686, total reward: 200.0\n",
            "Episode: 687, total reward: 200.0\n",
            "Episode: 688, total reward: 200.0\n",
            "Episode: 689, total reward: 200.0\n",
            "Episode: 690, total reward: 200.0\n",
            "Episode: 691, total reward: 200.0\n",
            "Episode: 692, total reward: 200.0\n",
            "Episode: 693, total reward: 167.0\n",
            "Episode: 694, total reward: 193.0\n",
            "Episode: 695, total reward: 200.0\n",
            "Episode: 696, total reward: 200.0\n",
            "Episode: 697, total reward: 200.0\n",
            "Episode: 698, total reward: 200.0\n",
            "Episode: 699, total reward: 200.0\n",
            "Episode: 700, total reward: 200.0\n",
            "Episode: 701, total reward: 200.0\n",
            "Episode: 702, total reward: 200.0\n",
            "Episode: 703, total reward: 200.0\n",
            "Episode: 704, total reward: 200.0\n",
            "Episode: 705, total reward: 200.0\n",
            "Episode: 706, total reward: 200.0\n",
            "Episode: 707, total reward: 200.0\n",
            "Episode: 708, total reward: 200.0\n",
            "Episode: 709, total reward: 171.0\n",
            "Episode: 710, total reward: 200.0\n",
            "Episode: 711, total reward: 200.0\n",
            "Episode: 712, total reward: 200.0\n",
            "Episode: 713, total reward: 200.0\n",
            "Episode: 714, total reward: 182.0\n",
            "Episode: 715, total reward: 188.0\n",
            "Episode: 716, total reward: 200.0\n",
            "Episode: 717, total reward: 200.0\n",
            "Episode: 718, total reward: 200.0\n",
            "Episode: 719, total reward: 192.0\n",
            "Episode: 720, total reward: 200.0\n",
            "Episode: 721, total reward: 200.0\n",
            "Episode: 722, total reward: 200.0\n",
            "Episode: 723, total reward: 182.0\n",
            "Episode: 724, total reward: 200.0\n",
            "Episode: 725, total reward: 200.0\n",
            "Episode: 726, total reward: 186.0\n",
            "Episode: 727, total reward: 200.0\n",
            "Episode: 728, total reward: 200.0\n",
            "Episode: 729, total reward: 200.0\n",
            "Episode: 730, total reward: 153.0\n",
            "Episode: 731, total reward: 200.0\n",
            "Episode: 732, total reward: 200.0\n",
            "Episode: 733, total reward: 200.0\n",
            "Episode: 734, total reward: 200.0\n",
            "Episode: 735, total reward: 200.0\n",
            "Episode: 736, total reward: 200.0\n",
            "Episode: 737, total reward: 163.0\n",
            "Episode: 738, total reward: 192.0\n",
            "Episode: 739, total reward: 200.0\n",
            "Episode: 740, total reward: 200.0\n",
            "Episode: 741, total reward: 195.0\n",
            "Episode: 742, total reward: 200.0\n",
            "Episode: 743, total reward: 198.0\n",
            "Episode: 744, total reward: 200.0\n",
            "Episode: 745, total reward: 180.0\n",
            "Episode: 746, total reward: 135.0\n",
            "Episode: 747, total reward: 177.0\n",
            "Episode: 748, total reward: 156.0\n",
            "Episode: 749, total reward: 154.0\n",
            "Episode: 750, total reward: 180.0\n",
            "Episode: 751, total reward: 200.0\n",
            "Episode: 752, total reward: 200.0\n",
            "Episode: 753, total reward: 195.0\n",
            "Episode: 754, total reward: 200.0\n",
            "Episode: 755, total reward: 200.0\n",
            "Episode: 756, total reward: 200.0\n",
            "Episode: 757, total reward: 200.0\n",
            "Episode: 758, total reward: 200.0\n",
            "Episode: 759, total reward: 196.0\n",
            "Episode: 760, total reward: 200.0\n",
            "Episode: 761, total reward: 200.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:445: UserWarning: Using a target size (torch.Size([196])) that is different to the input size (torch.Size([196, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Episode: 762, total reward: 200.0\n",
            "Episode: 763, total reward: 200.0\n",
            "Episode: 764, total reward: 200.0\n",
            "Episode: 765, total reward: 194.0\n",
            "Episode: 766, total reward: 200.0\n",
            "Episode: 767, total reward: 200.0\n",
            "Episode: 768, total reward: 200.0\n",
            "Episode: 769, total reward: 175.0\n",
            "Episode: 770, total reward: 194.0\n",
            "Episode: 771, total reward: 200.0\n",
            "Episode: 772, total reward: 200.0\n",
            "Episode: 773, total reward: 200.0\n",
            "Episode: 774, total reward: 200.0\n",
            "Episode: 775, total reward: 200.0\n",
            "Episode: 776, total reward: 200.0\n",
            "Episode: 777, total reward: 176.0\n",
            "Episode: 778, total reward: 179.0\n",
            "Episode: 779, total reward: 200.0\n",
            "Episode: 780, total reward: 200.0\n",
            "Episode: 781, total reward: 200.0\n",
            "Episode: 782, total reward: 200.0\n",
            "Episode: 783, total reward: 200.0\n",
            "Episode: 784, total reward: 200.0\n",
            "Episode: 785, total reward: 200.0\n",
            "Episode: 786, total reward: 200.0\n",
            "Episode: 787, total reward: 186.0\n",
            "Episode: 788, total reward: 200.0\n",
            "Episode: 789, total reward: 200.0\n",
            "Episode: 790, total reward: 200.0\n",
            "Episode: 791, total reward: 200.0\n",
            "Episode: 792, total reward: 200.0\n",
            "Episode: 793, total reward: 200.0\n",
            "Episode: 794, total reward: 200.0\n",
            "Episode: 795, total reward: 200.0\n",
            "Episode: 796, total reward: 200.0\n",
            "Episode: 797, total reward: 200.0\n",
            "Episode: 798, total reward: 200.0\n",
            "Episode: 799, total reward: 200.0\n",
            "Episode: 800, total reward: 200.0\n",
            "Episode: 801, total reward: 200.0\n",
            "Episode: 802, total reward: 200.0\n",
            "Episode: 803, total reward: 200.0\n",
            "Episode: 804, total reward: 200.0\n",
            "Episode: 805, total reward: 200.0\n",
            "Episode: 806, total reward: 200.0\n",
            "Episode: 807, total reward: 200.0\n",
            "Episode: 808, total reward: 200.0\n",
            "Episode: 809, total reward: 200.0\n",
            "Episode: 810, total reward: 200.0\n",
            "Episode: 811, total reward: 200.0\n",
            "Episode: 812, total reward: 200.0\n",
            "Episode: 813, total reward: 200.0\n",
            "Episode: 814, total reward: 200.0\n",
            "Episode: 815, total reward: 198.0\n",
            "Episode: 816, total reward: 200.0\n",
            "Episode: 817, total reward: 194.0\n",
            "Episode: 818, total reward: 200.0\n",
            "Episode: 819, total reward: 200.0\n",
            "Episode: 820, total reward: 200.0\n",
            "Episode: 821, total reward: 200.0\n",
            "Episode: 822, total reward: 200.0\n",
            "Episode: 823, total reward: 200.0\n",
            "Episode: 824, total reward: 200.0\n",
            "Episode: 825, total reward: 181.0\n",
            "Episode: 826, total reward: 196.0\n",
            "Episode: 827, total reward: 200.0\n",
            "Episode: 828, total reward: 200.0\n",
            "Episode: 829, total reward: 200.0\n",
            "Episode: 830, total reward: 200.0\n",
            "Episode: 831, total reward: 200.0\n",
            "Episode: 832, total reward: 200.0\n",
            "Episode: 833, total reward: 200.0\n",
            "Episode: 834, total reward: 200.0\n",
            "Episode: 835, total reward: 200.0\n",
            "Episode: 836, total reward: 200.0\n",
            "Episode: 837, total reward: 200.0\n",
            "Episode: 838, total reward: 200.0\n",
            "Episode: 839, total reward: 200.0\n",
            "Episode: 840, total reward: 200.0\n",
            "Episode: 841, total reward: 200.0\n",
            "Episode: 842, total reward: 200.0\n",
            "Episode: 843, total reward: 200.0\n",
            "Episode: 844, total reward: 200.0\n",
            "Episode: 845, total reward: 200.0\n",
            "Episode: 846, total reward: 200.0\n",
            "Episode: 847, total reward: 200.0\n",
            "Episode: 848, total reward: 200.0\n",
            "Episode: 849, total reward: 200.0\n",
            "Episode: 850, total reward: 200.0\n",
            "Episode: 851, total reward: 200.0\n",
            "Episode: 852, total reward: 200.0\n",
            "Episode: 853, total reward: 200.0\n",
            "Episode: 854, total reward: 200.0\n",
            "Episode: 855, total reward: 195.0\n",
            "Episode: 856, total reward: 200.0\n",
            "Episode: 857, total reward: 200.0\n",
            "Episode: 858, total reward: 200.0\n",
            "Episode: 859, total reward: 200.0\n",
            "Episode: 860, total reward: 200.0\n",
            "Episode: 861, total reward: 200.0\n",
            "Episode: 862, total reward: 200.0\n",
            "Episode: 863, total reward: 200.0\n",
            "Episode: 864, total reward: 200.0\n",
            "Episode: 865, total reward: 200.0\n",
            "Episode: 866, total reward: 200.0\n",
            "Episode: 867, total reward: 200.0\n",
            "Episode: 868, total reward: 200.0\n",
            "Episode: 869, total reward: 200.0\n",
            "Episode: 870, total reward: 200.0\n",
            "Episode: 871, total reward: 200.0\n",
            "Episode: 872, total reward: 200.0\n",
            "Episode: 873, total reward: 200.0\n",
            "Episode: 874, total reward: 200.0\n",
            "Episode: 875, total reward: 200.0\n",
            "Episode: 876, total reward: 200.0\n",
            "Episode: 877, total reward: 200.0\n",
            "Episode: 878, total reward: 200.0\n",
            "Episode: 879, total reward: 200.0\n",
            "Episode: 880, total reward: 200.0\n",
            "Episode: 881, total reward: 200.0\n",
            "Episode: 882, total reward: 200.0\n",
            "Episode: 883, total reward: 200.0\n",
            "Episode: 884, total reward: 200.0\n",
            "Episode: 885, total reward: 200.0\n",
            "Episode: 886, total reward: 200.0\n",
            "Episode: 887, total reward: 200.0\n",
            "Episode: 888, total reward: 200.0\n",
            "Episode: 889, total reward: 200.0\n",
            "Episode: 890, total reward: 200.0\n",
            "Episode: 891, total reward: 200.0\n",
            "Episode: 892, total reward: 178.0\n",
            "Episode: 893, total reward: 200.0\n",
            "Episode: 894, total reward: 200.0\n",
            "Episode: 895, total reward: 200.0\n",
            "Episode: 896, total reward: 200.0\n",
            "Episode: 897, total reward: 200.0\n",
            "Episode: 898, total reward: 200.0\n",
            "Episode: 899, total reward: 200.0\n",
            "Episode: 900, total reward: 200.0\n",
            "Episode: 901, total reward: 200.0\n",
            "Episode: 902, total reward: 200.0\n",
            "Episode: 903, total reward: 200.0\n",
            "Episode: 904, total reward: 200.0\n",
            "Episode: 905, total reward: 200.0\n",
            "Episode: 906, total reward: 200.0\n",
            "Episode: 907, total reward: 200.0\n",
            "Episode: 908, total reward: 200.0\n",
            "Episode: 909, total reward: 198.0\n",
            "Episode: 910, total reward: 200.0\n",
            "Episode: 911, total reward: 200.0\n",
            "Episode: 912, total reward: 200.0\n",
            "Episode: 913, total reward: 200.0\n",
            "Episode: 914, total reward: 200.0\n",
            "Episode: 915, total reward: 200.0\n",
            "Episode: 916, total reward: 200.0\n",
            "Episode: 917, total reward: 200.0\n",
            "Episode: 918, total reward: 200.0\n",
            "Episode: 919, total reward: 200.0\n",
            "Episode: 920, total reward: 200.0\n",
            "Episode: 921, total reward: 200.0\n",
            "Episode: 922, total reward: 200.0\n",
            "Episode: 923, total reward: 196.0\n",
            "Episode: 924, total reward: 200.0\n",
            "Episode: 925, total reward: 200.0\n",
            "Episode: 926, total reward: 200.0\n",
            "Episode: 927, total reward: 200.0\n",
            "Episode: 928, total reward: 200.0\n",
            "Episode: 929, total reward: 200.0\n",
            "Episode: 930, total reward: 200.0\n",
            "Episode: 931, total reward: 200.0\n",
            "Episode: 932, total reward: 200.0\n",
            "Episode: 933, total reward: 200.0\n",
            "Episode: 934, total reward: 200.0\n",
            "Episode: 935, total reward: 200.0\n",
            "Episode: 936, total reward: 200.0\n",
            "Episode: 937, total reward: 200.0\n",
            "Episode: 938, total reward: 200.0\n",
            "Episode: 939, total reward: 200.0\n",
            "Episode: 940, total reward: 200.0\n",
            "Episode: 941, total reward: 200.0\n",
            "Episode: 942, total reward: 200.0\n",
            "Episode: 943, total reward: 200.0\n",
            "Episode: 944, total reward: 200.0\n",
            "Episode: 945, total reward: 200.0\n",
            "Episode: 946, total reward: 200.0\n",
            "Episode: 947, total reward: 200.0\n",
            "Episode: 948, total reward: 200.0\n",
            "Episode: 949, total reward: 200.0\n",
            "Episode: 950, total reward: 200.0\n",
            "Episode: 951, total reward: 200.0\n",
            "Episode: 952, total reward: 200.0\n",
            "Episode: 953, total reward: 200.0\n",
            "Episode: 954, total reward: 200.0\n",
            "Episode: 955, total reward: 200.0\n",
            "Episode: 956, total reward: 200.0\n",
            "Episode: 957, total reward: 200.0\n",
            "Episode: 958, total reward: 200.0\n",
            "Episode: 959, total reward: 200.0\n",
            "Episode: 960, total reward: 200.0\n",
            "Episode: 961, total reward: 200.0\n",
            "Episode: 962, total reward: 200.0\n",
            "Episode: 963, total reward: 200.0\n",
            "Episode: 964, total reward: 186.0\n",
            "Episode: 965, total reward: 192.0\n",
            "Episode: 966, total reward: 200.0\n",
            "Episode: 967, total reward: 200.0\n",
            "Episode: 968, total reward: 200.0\n",
            "Episode: 969, total reward: 200.0\n",
            "Episode: 970, total reward: 200.0\n",
            "Episode: 971, total reward: 200.0\n",
            "Episode: 972, total reward: 200.0\n",
            "Episode: 973, total reward: 200.0\n",
            "Episode: 974, total reward: 200.0\n",
            "Episode: 975, total reward: 200.0\n",
            "Episode: 976, total reward: 200.0\n",
            "Episode: 977, total reward: 200.0\n",
            "Episode: 978, total reward: 200.0\n",
            "Episode: 979, total reward: 176.0\n",
            "Episode: 980, total reward: 200.0\n",
            "Episode: 981, total reward: 200.0\n",
            "Episode: 982, total reward: 196.0\n",
            "Episode: 983, total reward: 200.0\n",
            "Episode: 984, total reward: 200.0\n",
            "Episode: 985, total reward: 200.0\n",
            "Episode: 986, total reward: 200.0\n",
            "Episode: 987, total reward: 200.0\n",
            "Episode: 988, total reward: 200.0\n",
            "Episode: 989, total reward: 200.0\n",
            "Episode: 990, total reward: 200.0\n",
            "Episode: 991, total reward: 200.0\n",
            "Episode: 992, total reward: 200.0\n",
            "Episode: 993, total reward: 200.0\n",
            "Episode: 994, total reward: 200.0\n",
            "Episode: 995, total reward: 200.0\n",
            "Episode: 996, total reward: 200.0\n",
            "Episode: 997, total reward: 200.0\n",
            "Episode: 998, total reward: 200.0\n",
            "Episode: 999, total reward: 200.0\n",
            "Episode: 1000, total reward: 200.0\n",
            "Episode: 1001, total reward: 200.0\n",
            "Episode: 1002, total reward: 200.0\n",
            "Episode: 1003, total reward: 200.0\n",
            "Episode: 1004, total reward: 200.0\n",
            "Episode: 1005, total reward: 200.0\n",
            "Episode: 1006, total reward: 200.0\n",
            "Episode: 1007, total reward: 200.0\n",
            "Episode: 1008, total reward: 200.0\n",
            "Episode: 1009, total reward: 200.0\n",
            "Episode: 1010, total reward: 200.0\n",
            "Episode: 1011, total reward: 200.0\n",
            "Episode: 1012, total reward: 200.0\n",
            "Episode: 1013, total reward: 200.0\n",
            "Episode: 1014, total reward: 200.0\n",
            "Episode: 1015, total reward: 200.0\n",
            "Episode: 1016, total reward: 200.0\n",
            "Episode: 1017, total reward: 200.0\n",
            "Episode: 1018, total reward: 200.0\n",
            "Episode: 1019, total reward: 200.0\n",
            "Episode: 1020, total reward: 200.0\n",
            "Episode: 1021, total reward: 200.0\n",
            "Episode: 1022, total reward: 200.0\n",
            "Episode: 1023, total reward: 200.0\n",
            "Episode: 1024, total reward: 200.0\n",
            "Episode: 1025, total reward: 200.0\n",
            "Episode: 1026, total reward: 200.0\n",
            "Episode: 1027, total reward: 200.0\n",
            "Episode: 1028, total reward: 200.0\n",
            "Episode: 1029, total reward: 200.0\n",
            "Episode: 1030, total reward: 200.0\n",
            "Episode: 1031, total reward: 200.0\n",
            "Episode: 1032, total reward: 200.0\n",
            "Episode: 1033, total reward: 173.0\n",
            "Episode: 1034, total reward: 200.0\n",
            "Episode: 1035, total reward: 200.0\n",
            "Episode: 1036, total reward: 200.0\n",
            "Episode: 1037, total reward: 200.0\n",
            "Episode: 1038, total reward: 200.0\n",
            "Episode: 1039, total reward: 200.0\n",
            "Episode: 1040, total reward: 200.0\n",
            "Episode: 1041, total reward: 200.0\n",
            "Episode: 1042, total reward: 200.0\n",
            "Episode: 1043, total reward: 200.0\n",
            "Episode: 1044, total reward: 200.0\n",
            "Episode: 1045, total reward: 200.0\n",
            "Episode: 1046, total reward: 200.0\n",
            "Episode: 1047, total reward: 200.0\n",
            "Episode: 1048, total reward: 200.0\n",
            "Episode: 1049, total reward: 200.0\n",
            "Episode: 1050, total reward: 200.0\n",
            "Episode: 1051, total reward: 200.0\n",
            "Episode: 1052, total reward: 200.0\n",
            "Episode: 1053, total reward: 200.0\n",
            "Episode: 1054, total reward: 200.0\n",
            "Episode: 1055, total reward: 200.0\n",
            "Episode: 1056, total reward: 200.0\n",
            "Episode: 1057, total reward: 200.0\n",
            "Episode: 1058, total reward: 200.0\n",
            "Episode: 1059, total reward: 200.0\n",
            "Episode: 1060, total reward: 200.0\n",
            "Episode: 1061, total reward: 200.0\n",
            "Episode: 1062, total reward: 200.0\n",
            "Episode: 1063, total reward: 200.0\n",
            "Episode: 1064, total reward: 200.0\n",
            "Episode: 1065, total reward: 200.0\n",
            "Episode: 1066, total reward: 200.0\n",
            "Episode: 1067, total reward: 200.0\n",
            "Episode: 1068, total reward: 200.0\n",
            "Episode: 1069, total reward: 200.0\n",
            "Episode: 1070, total reward: 200.0\n",
            "Episode: 1071, total reward: 200.0\n",
            "Episode: 1072, total reward: 200.0\n",
            "Episode: 1073, total reward: 200.0\n",
            "Episode: 1074, total reward: 200.0\n",
            "Episode: 1075, total reward: 200.0\n",
            "Episode: 1076, total reward: 200.0\n",
            "Episode: 1077, total reward: 200.0\n",
            "Episode: 1078, total reward: 200.0\n",
            "Episode: 1079, total reward: 200.0\n",
            "Episode: 1080, total reward: 200.0\n",
            "Episode: 1081, total reward: 200.0\n",
            "Episode: 1082, total reward: 200.0\n",
            "Episode: 1083, total reward: 200.0\n",
            "Episode: 1084, total reward: 200.0\n",
            "Episode: 1085, total reward: 200.0\n",
            "Episode: 1086, total reward: 200.0\n",
            "Episode: 1087, total reward: 200.0\n",
            "Episode: 1088, total reward: 200.0\n",
            "Episode: 1089, total reward: 200.0\n",
            "Episode: 1090, total reward: 200.0\n",
            "Episode: 1091, total reward: 200.0\n",
            "Episode: 1092, total reward: 200.0\n",
            "Episode: 1093, total reward: 200.0\n",
            "Episode: 1094, total reward: 200.0\n",
            "Episode: 1095, total reward: 200.0\n",
            "Episode: 1096, total reward: 200.0\n",
            "Episode: 1097, total reward: 200.0\n",
            "Episode: 1098, total reward: 200.0\n",
            "Episode: 1099, total reward: 200.0\n",
            "Episode: 1100, total reward: 200.0\n",
            "Episode: 1101, total reward: 200.0\n",
            "Episode: 1102, total reward: 200.0\n",
            "Episode: 1103, total reward: 200.0\n",
            "Episode: 1104, total reward: 200.0\n",
            "Episode: 1105, total reward: 200.0\n",
            "Episode: 1106, total reward: 200.0\n",
            "Episode: 1107, total reward: 200.0\n",
            "Episode: 1108, total reward: 200.0\n",
            "Episode: 1109, total reward: 200.0\n",
            "Episode: 1110, total reward: 200.0\n",
            "Episode: 1111, total reward: 200.0\n",
            "Episode: 1112, total reward: 200.0\n",
            "Episode: 1113, total reward: 200.0\n",
            "Episode: 1114, total reward: 200.0\n",
            "Episode: 1115, total reward: 200.0\n",
            "Episode: 1116, total reward: 200.0\n",
            "Episode: 1117, total reward: 200.0\n",
            "Episode: 1118, total reward: 200.0\n",
            "Episode: 1119, total reward: 200.0\n",
            "Episode: 1120, total reward: 200.0\n",
            "Episode: 1121, total reward: 200.0\n",
            "Episode: 1122, total reward: 200.0\n",
            "Episode: 1123, total reward: 200.0\n",
            "Episode: 1124, total reward: 200.0\n",
            "Episode: 1125, total reward: 200.0\n",
            "Episode: 1126, total reward: 164.0\n",
            "Episode: 1127, total reward: 200.0\n",
            "Episode: 1128, total reward: 200.0\n",
            "Episode: 1129, total reward: 200.0\n",
            "Episode: 1130, total reward: 200.0\n",
            "Episode: 1131, total reward: 200.0\n",
            "Episode: 1132, total reward: 200.0\n",
            "Episode: 1133, total reward: 200.0\n",
            "Episode: 1134, total reward: 200.0\n",
            "Episode: 1135, total reward: 200.0\n",
            "Episode: 1136, total reward: 200.0\n",
            "Episode: 1137, total reward: 200.0\n",
            "Episode: 1138, total reward: 200.0\n",
            "Episode: 1139, total reward: 200.0\n",
            "Episode: 1140, total reward: 200.0\n",
            "Episode: 1141, total reward: 200.0\n",
            "Episode: 1142, total reward: 200.0\n",
            "Episode: 1143, total reward: 200.0\n",
            "Episode: 1144, total reward: 200.0\n",
            "Episode: 1145, total reward: 200.0\n",
            "Episode: 1146, total reward: 200.0\n",
            "Episode: 1147, total reward: 200.0\n",
            "Episode: 1148, total reward: 200.0\n",
            "Episode: 1149, total reward: 200.0\n",
            "Episode: 1150, total reward: 200.0\n",
            "Episode: 1151, total reward: 200.0\n",
            "Episode: 1152, total reward: 200.0\n",
            "Episode: 1153, total reward: 200.0\n",
            "Episode: 1154, total reward: 200.0\n",
            "Episode: 1155, total reward: 200.0\n",
            "Episode: 1156, total reward: 200.0\n",
            "Episode: 1157, total reward: 200.0\n",
            "Episode: 1158, total reward: 200.0\n",
            "Episode: 1159, total reward: 200.0\n",
            "Episode: 1160, total reward: 200.0\n",
            "Episode: 1161, total reward: 200.0\n",
            "Episode: 1162, total reward: 200.0\n",
            "Episode: 1163, total reward: 200.0\n",
            "Episode: 1164, total reward: 200.0\n",
            "Episode: 1165, total reward: 200.0\n",
            "Episode: 1166, total reward: 200.0\n",
            "Episode: 1167, total reward: 200.0\n",
            "Episode: 1168, total reward: 200.0\n",
            "Episode: 1169, total reward: 200.0\n",
            "Episode: 1170, total reward: 200.0\n",
            "Episode: 1171, total reward: 200.0\n",
            "Episode: 1172, total reward: 200.0\n",
            "Episode: 1173, total reward: 200.0\n",
            "Episode: 1174, total reward: 200.0\n",
            "Episode: 1175, total reward: 200.0\n",
            "Episode: 1176, total reward: 200.0\n",
            "Episode: 1177, total reward: 200.0\n",
            "Episode: 1178, total reward: 200.0\n",
            "Episode: 1179, total reward: 200.0\n",
            "Episode: 1180, total reward: 200.0\n",
            "Episode: 1181, total reward: 200.0\n",
            "Episode: 1182, total reward: 200.0\n",
            "Episode: 1183, total reward: 200.0\n",
            "Episode: 1184, total reward: 200.0\n",
            "Episode: 1185, total reward: 200.0\n",
            "Episode: 1186, total reward: 200.0\n",
            "Episode: 1187, total reward: 200.0\n",
            "Episode: 1188, total reward: 200.0\n",
            "Episode: 1189, total reward: 200.0\n",
            "Episode: 1190, total reward: 200.0\n",
            "Episode: 1191, total reward: 200.0\n",
            "Episode: 1192, total reward: 200.0\n",
            "Episode: 1193, total reward: 200.0\n",
            "Episode: 1194, total reward: 200.0\n",
            "Episode: 1195, total reward: 200.0\n",
            "Episode: 1196, total reward: 200.0\n",
            "Episode: 1197, total reward: 200.0\n",
            "Episode: 1198, total reward: 200.0\n",
            "Episode: 1199, total reward: 200.0\n",
            "Episode: 1200, total reward: 200.0\n",
            "Episode: 1201, total reward: 200.0\n",
            "Episode: 1202, total reward: 200.0\n",
            "Episode: 1203, total reward: 200.0\n",
            "Episode: 1204, total reward: 200.0\n",
            "Episode: 1205, total reward: 200.0\n",
            "Episode: 1206, total reward: 200.0\n",
            "Episode: 1207, total reward: 200.0\n",
            "Episode: 1208, total reward: 200.0\n",
            "Episode: 1209, total reward: 200.0\n",
            "Episode: 1210, total reward: 200.0\n",
            "Episode: 1211, total reward: 200.0\n",
            "Episode: 1212, total reward: 200.0\n",
            "Episode: 1213, total reward: 200.0\n",
            "Episode: 1214, total reward: 200.0\n",
            "Episode: 1215, total reward: 200.0\n",
            "Episode: 1216, total reward: 200.0\n",
            "Episode: 1217, total reward: 200.0\n",
            "Episode: 1218, total reward: 200.0\n",
            "Episode: 1219, total reward: 200.0\n",
            "Episode: 1220, total reward: 200.0\n",
            "Episode: 1221, total reward: 200.0\n",
            "Episode: 1222, total reward: 200.0\n",
            "Episode: 1223, total reward: 200.0\n",
            "Episode: 1224, total reward: 200.0\n",
            "Episode: 1225, total reward: 181.0\n",
            "Episode: 1226, total reward: 200.0\n",
            "Episode: 1227, total reward: 200.0\n",
            "Episode: 1228, total reward: 200.0\n",
            "Episode: 1229, total reward: 200.0\n",
            "Episode: 1230, total reward: 200.0\n",
            "Episode: 1231, total reward: 200.0\n",
            "Episode: 1232, total reward: 200.0\n",
            "Episode: 1233, total reward: 200.0\n",
            "Episode: 1234, total reward: 200.0\n",
            "Episode: 1235, total reward: 200.0\n",
            "Episode: 1236, total reward: 200.0\n",
            "Episode: 1237, total reward: 200.0\n",
            "Episode: 1238, total reward: 200.0\n",
            "Episode: 1239, total reward: 200.0\n",
            "Episode: 1240, total reward: 200.0\n",
            "Episode: 1241, total reward: 200.0\n",
            "Episode: 1242, total reward: 200.0\n",
            "Episode: 1243, total reward: 200.0\n",
            "Episode: 1244, total reward: 200.0\n",
            "Episode: 1245, total reward: 200.0\n",
            "Episode: 1246, total reward: 200.0\n",
            "Episode: 1247, total reward: 200.0\n",
            "Episode: 1248, total reward: 200.0\n",
            "Episode: 1249, total reward: 200.0\n",
            "Episode: 1250, total reward: 200.0\n",
            "Episode: 1251, total reward: 200.0\n",
            "Episode: 1252, total reward: 200.0\n",
            "Episode: 1253, total reward: 200.0\n",
            "Episode: 1254, total reward: 200.0\n",
            "Episode: 1255, total reward: 200.0\n",
            "Episode: 1256, total reward: 200.0\n",
            "Episode: 1257, total reward: 200.0\n",
            "Episode: 1258, total reward: 200.0\n",
            "Episode: 1259, total reward: 200.0\n",
            "Episode: 1260, total reward: 200.0\n",
            "Episode: 1261, total reward: 200.0\n",
            "Episode: 1262, total reward: 200.0\n",
            "Episode: 1263, total reward: 200.0\n",
            "Episode: 1264, total reward: 200.0\n",
            "Episode: 1265, total reward: 200.0\n",
            "Episode: 1266, total reward: 200.0\n",
            "Episode: 1267, total reward: 200.0\n",
            "Episode: 1268, total reward: 200.0\n",
            "Episode: 1269, total reward: 200.0\n",
            "Episode: 1270, total reward: 200.0\n",
            "Episode: 1271, total reward: 200.0\n",
            "Episode: 1272, total reward: 200.0\n",
            "Episode: 1273, total reward: 200.0\n",
            "Episode: 1274, total reward: 200.0\n",
            "Episode: 1275, total reward: 200.0\n",
            "Episode: 1276, total reward: 200.0\n",
            "Episode: 1277, total reward: 200.0\n",
            "Episode: 1278, total reward: 200.0\n",
            "Episode: 1279, total reward: 200.0\n",
            "Episode: 1280, total reward: 200.0\n",
            "Episode: 1281, total reward: 200.0\n",
            "Episode: 1282, total reward: 200.0\n",
            "Episode: 1283, total reward: 200.0\n",
            "Episode: 1284, total reward: 200.0\n",
            "Episode: 1285, total reward: 200.0\n",
            "Episode: 1286, total reward: 200.0\n",
            "Episode: 1287, total reward: 200.0\n",
            "Episode: 1288, total reward: 200.0\n",
            "Episode: 1289, total reward: 200.0\n",
            "Episode: 1290, total reward: 200.0\n",
            "Episode: 1291, total reward: 200.0\n",
            "Episode: 1292, total reward: 200.0\n",
            "Episode: 1293, total reward: 200.0\n",
            "Episode: 1294, total reward: 200.0\n",
            "Episode: 1295, total reward: 200.0\n",
            "Episode: 1296, total reward: 200.0\n",
            "Episode: 1297, total reward: 200.0\n",
            "Episode: 1298, total reward: 200.0\n",
            "Episode: 1299, total reward: 200.0\n",
            "Episode: 1300, total reward: 200.0\n",
            "Episode: 1301, total reward: 200.0\n",
            "Episode: 1302, total reward: 200.0\n",
            "Episode: 1303, total reward: 200.0\n",
            "Episode: 1304, total reward: 200.0\n",
            "Episode: 1305, total reward: 200.0\n",
            "Episode: 1306, total reward: 200.0\n",
            "Episode: 1307, total reward: 200.0\n",
            "Episode: 1308, total reward: 200.0\n",
            "Episode: 1309, total reward: 200.0\n",
            "Episode: 1310, total reward: 200.0\n",
            "Episode: 1311, total reward: 200.0\n",
            "Episode: 1312, total reward: 200.0\n",
            "Episode: 1313, total reward: 200.0\n",
            "Episode: 1314, total reward: 200.0\n",
            "Episode: 1315, total reward: 200.0\n",
            "Episode: 1316, total reward: 200.0\n",
            "Episode: 1317, total reward: 200.0\n",
            "Episode: 1318, total reward: 200.0\n",
            "Episode: 1319, total reward: 200.0\n",
            "Episode: 1320, total reward: 200.0\n",
            "Episode: 1321, total reward: 200.0\n",
            "Episode: 1322, total reward: 153.0\n",
            "Episode: 1323, total reward: 180.0\n",
            "Episode: 1324, total reward: 200.0\n",
            "Episode: 1325, total reward: 200.0\n",
            "Episode: 1326, total reward: 200.0\n",
            "Episode: 1327, total reward: 200.0\n",
            "Episode: 1328, total reward: 200.0\n",
            "Episode: 1329, total reward: 200.0\n",
            "Episode: 1330, total reward: 200.0\n",
            "Episode: 1331, total reward: 200.0\n",
            "Episode: 1332, total reward: 200.0\n",
            "Episode: 1333, total reward: 200.0\n",
            "Episode: 1334, total reward: 197.0\n",
            "Episode: 1335, total reward: 200.0\n",
            "Episode: 1336, total reward: 200.0\n",
            "Episode: 1337, total reward: 200.0\n",
            "Episode: 1338, total reward: 200.0\n",
            "Episode: 1339, total reward: 200.0\n",
            "Episode: 1340, total reward: 200.0\n",
            "Episode: 1341, total reward: 200.0\n",
            "Episode: 1342, total reward: 200.0\n",
            "Episode: 1343, total reward: 200.0\n",
            "Episode: 1344, total reward: 200.0\n",
            "Episode: 1345, total reward: 163.0\n",
            "Episode: 1346, total reward: 200.0\n",
            "Episode: 1347, total reward: 200.0\n",
            "Episode: 1348, total reward: 200.0\n",
            "Episode: 1349, total reward: 200.0\n",
            "Episode: 1350, total reward: 200.0\n",
            "Episode: 1351, total reward: 200.0\n",
            "Episode: 1352, total reward: 200.0\n",
            "Episode: 1353, total reward: 200.0\n",
            "Episode: 1354, total reward: 200.0\n",
            "Episode: 1355, total reward: 200.0\n",
            "Episode: 1356, total reward: 200.0\n",
            "Episode: 1357, total reward: 200.0\n",
            "Episode: 1358, total reward: 200.0\n",
            "Episode: 1359, total reward: 200.0\n",
            "Episode: 1360, total reward: 200.0\n",
            "Episode: 1361, total reward: 200.0\n",
            "Episode: 1362, total reward: 200.0\n",
            "Episode: 1363, total reward: 200.0\n",
            "Episode: 1364, total reward: 200.0\n",
            "Episode: 1365, total reward: 200.0\n",
            "Episode: 1366, total reward: 200.0\n",
            "Episode: 1367, total reward: 200.0\n",
            "Episode: 1368, total reward: 200.0\n",
            "Episode: 1369, total reward: 200.0\n",
            "Episode: 1370, total reward: 200.0\n",
            "Episode: 1371, total reward: 200.0\n",
            "Episode: 1372, total reward: 200.0\n",
            "Episode: 1373, total reward: 200.0\n",
            "Episode: 1374, total reward: 200.0\n",
            "Episode: 1375, total reward: 200.0\n",
            "Episode: 1376, total reward: 200.0\n",
            "Episode: 1377, total reward: 200.0\n",
            "Episode: 1378, total reward: 200.0\n",
            "Episode: 1379, total reward: 200.0\n",
            "Episode: 1380, total reward: 200.0\n",
            "Episode: 1381, total reward: 200.0\n",
            "Episode: 1382, total reward: 200.0\n",
            "Episode: 1383, total reward: 200.0\n",
            "Episode: 1384, total reward: 200.0\n",
            "Episode: 1385, total reward: 200.0\n",
            "Episode: 1386, total reward: 200.0\n",
            "Episode: 1387, total reward: 200.0\n",
            "Episode: 1388, total reward: 200.0\n",
            "Episode: 1389, total reward: 200.0\n",
            "Episode: 1390, total reward: 200.0\n",
            "Episode: 1391, total reward: 200.0\n",
            "Episode: 1392, total reward: 200.0\n",
            "Episode: 1393, total reward: 200.0\n",
            "Episode: 1394, total reward: 200.0\n",
            "Episode: 1395, total reward: 200.0\n",
            "Episode: 1396, total reward: 200.0\n",
            "Episode: 1397, total reward: 200.0\n",
            "Episode: 1398, total reward: 200.0\n",
            "Episode: 1399, total reward: 200.0\n",
            "Episode: 1400, total reward: 200.0\n",
            "Episode: 1401, total reward: 200.0\n",
            "Episode: 1402, total reward: 200.0\n",
            "Episode: 1403, total reward: 200.0\n",
            "Episode: 1404, total reward: 200.0\n",
            "Episode: 1405, total reward: 200.0\n",
            "Episode: 1406, total reward: 200.0\n",
            "Episode: 1407, total reward: 200.0\n",
            "Episode: 1408, total reward: 200.0\n",
            "Episode: 1409, total reward: 200.0\n",
            "Episode: 1410, total reward: 200.0\n",
            "Episode: 1411, total reward: 200.0\n",
            "Episode: 1412, total reward: 200.0\n",
            "Episode: 1413, total reward: 200.0\n",
            "Episode: 1414, total reward: 200.0\n",
            "Episode: 1415, total reward: 200.0\n",
            "Episode: 1416, total reward: 200.0\n",
            "Episode: 1417, total reward: 200.0\n",
            "Episode: 1418, total reward: 200.0\n",
            "Episode: 1419, total reward: 200.0\n",
            "Episode: 1420, total reward: 200.0\n",
            "Episode: 1421, total reward: 200.0\n",
            "Episode: 1422, total reward: 200.0\n",
            "Episode: 1423, total reward: 200.0\n",
            "Episode: 1424, total reward: 200.0\n",
            "Episode: 1425, total reward: 200.0\n",
            "Episode: 1426, total reward: 200.0\n",
            "Episode: 1427, total reward: 200.0\n",
            "Episode: 1428, total reward: 200.0\n",
            "Episode: 1429, total reward: 200.0\n",
            "Episode: 1430, total reward: 200.0\n",
            "Episode: 1431, total reward: 200.0\n",
            "Episode: 1432, total reward: 200.0\n",
            "Episode: 1433, total reward: 200.0\n",
            "Episode: 1434, total reward: 200.0\n",
            "Episode: 1435, total reward: 200.0\n",
            "Episode: 1436, total reward: 200.0\n",
            "Episode: 1437, total reward: 200.0\n",
            "Episode: 1438, total reward: 200.0\n",
            "Episode: 1439, total reward: 200.0\n",
            "Episode: 1440, total reward: 200.0\n",
            "Episode: 1441, total reward: 200.0\n",
            "Episode: 1442, total reward: 200.0\n",
            "Episode: 1443, total reward: 200.0\n",
            "Episode: 1444, total reward: 200.0\n",
            "Episode: 1445, total reward: 200.0\n",
            "Episode: 1446, total reward: 200.0\n",
            "Episode: 1447, total reward: 200.0\n",
            "Episode: 1448, total reward: 200.0\n",
            "Episode: 1449, total reward: 200.0\n",
            "Episode: 1450, total reward: 200.0\n",
            "Episode: 1451, total reward: 200.0\n",
            "Episode: 1452, total reward: 200.0\n",
            "Episode: 1453, total reward: 200.0\n",
            "Episode: 1454, total reward: 200.0\n",
            "Episode: 1455, total reward: 200.0\n",
            "Episode: 1456, total reward: 200.0\n",
            "Episode: 1457, total reward: 200.0\n",
            "Episode: 1458, total reward: 200.0\n",
            "Episode: 1459, total reward: 200.0\n",
            "Episode: 1460, total reward: 200.0\n",
            "Episode: 1461, total reward: 200.0\n",
            "Episode: 1462, total reward: 200.0\n",
            "Episode: 1463, total reward: 200.0\n",
            "Episode: 1464, total reward: 200.0\n",
            "Episode: 1465, total reward: 200.0\n",
            "Episode: 1466, total reward: 200.0\n",
            "Episode: 1467, total reward: 200.0\n",
            "Episode: 1468, total reward: 200.0\n",
            "Episode: 1469, total reward: 200.0\n",
            "Episode: 1470, total reward: 200.0\n",
            "Episode: 1471, total reward: 200.0\n",
            "Episode: 1472, total reward: 200.0\n",
            "Episode: 1473, total reward: 200.0\n",
            "Episode: 1474, total reward: 200.0\n",
            "Episode: 1475, total reward: 200.0\n",
            "Episode: 1476, total reward: 200.0\n",
            "Episode: 1477, total reward: 200.0\n",
            "Episode: 1478, total reward: 200.0\n",
            "Episode: 1479, total reward: 200.0\n",
            "Episode: 1480, total reward: 200.0\n",
            "Episode: 1481, total reward: 200.0\n",
            "Episode: 1482, total reward: 200.0\n",
            "Episode: 1483, total reward: 200.0\n",
            "Episode: 1484, total reward: 200.0\n",
            "Episode: 1485, total reward: 200.0\n",
            "Episode: 1486, total reward: 200.0\n",
            "Episode: 1487, total reward: 200.0\n",
            "Episode: 1488, total reward: 200.0\n",
            "Episode: 1489, total reward: 200.0\n",
            "Episode: 1490, total reward: 200.0\n",
            "Episode: 1491, total reward: 200.0\n",
            "Episode: 1492, total reward: 200.0\n",
            "Episode: 1493, total reward: 200.0\n",
            "Episode: 1494, total reward: 200.0\n",
            "Episode: 1495, total reward: 200.0\n",
            "Episode: 1496, total reward: 200.0\n",
            "Episode: 1497, total reward: 200.0\n",
            "Episode: 1498, total reward: 200.0\n",
            "Episode: 1499, total reward: 200.0\n",
            "Episode: 1500, total reward: 200.0\n",
            "Episode: 1501, total reward: 200.0\n",
            "Episode: 1502, total reward: 200.0\n",
            "Episode: 1503, total reward: 200.0\n",
            "Episode: 1504, total reward: 200.0\n",
            "Episode: 1505, total reward: 200.0\n",
            "Episode: 1506, total reward: 200.0\n",
            "Episode: 1507, total reward: 200.0\n",
            "Episode: 1508, total reward: 200.0\n",
            "Episode: 1509, total reward: 200.0\n",
            "Episode: 1510, total reward: 200.0\n",
            "Episode: 1511, total reward: 200.0\n",
            "Episode: 1512, total reward: 200.0\n",
            "Episode: 1513, total reward: 200.0\n",
            "Episode: 1514, total reward: 200.0\n",
            "Episode: 1515, total reward: 200.0\n",
            "Episode: 1516, total reward: 200.0\n",
            "Episode: 1517, total reward: 200.0\n",
            "Episode: 1518, total reward: 200.0\n",
            "Episode: 1519, total reward: 200.0\n",
            "Episode: 1520, total reward: 200.0\n",
            "Episode: 1521, total reward: 200.0\n",
            "Episode: 1522, total reward: 200.0\n",
            "Episode: 1523, total reward: 200.0\n",
            "Episode: 1524, total reward: 200.0\n",
            "Episode: 1525, total reward: 200.0\n",
            "Episode: 1526, total reward: 200.0\n",
            "Episode: 1527, total reward: 200.0\n",
            "Episode: 1528, total reward: 200.0\n",
            "Episode: 1529, total reward: 200.0\n",
            "Episode: 1530, total reward: 200.0\n",
            "Episode: 1531, total reward: 200.0\n",
            "Episode: 1532, total reward: 200.0\n",
            "Episode: 1533, total reward: 200.0\n",
            "Episode: 1534, total reward: 200.0\n",
            "Episode: 1535, total reward: 200.0\n",
            "Episode: 1536, total reward: 200.0\n",
            "Episode: 1537, total reward: 200.0\n",
            "Episode: 1538, total reward: 200.0\n",
            "Episode: 1539, total reward: 200.0\n",
            "Episode: 1540, total reward: 200.0\n",
            "Episode: 1541, total reward: 200.0\n",
            "Episode: 1542, total reward: 200.0\n",
            "Episode: 1543, total reward: 200.0\n",
            "Episode: 1544, total reward: 200.0\n",
            "Episode: 1545, total reward: 200.0\n",
            "Episode: 1546, total reward: 200.0\n",
            "Episode: 1547, total reward: 200.0\n",
            "Episode: 1548, total reward: 200.0\n",
            "Episode: 1549, total reward: 200.0\n",
            "Episode: 1550, total reward: 200.0\n",
            "Episode: 1551, total reward: 200.0\n",
            "Episode: 1552, total reward: 200.0\n",
            "Episode: 1553, total reward: 200.0\n",
            "Episode: 1554, total reward: 200.0\n",
            "Episode: 1555, total reward: 200.0\n",
            "Episode: 1556, total reward: 200.0\n",
            "Episode: 1557, total reward: 200.0\n",
            "Episode: 1558, total reward: 200.0\n",
            "Episode: 1559, total reward: 200.0\n",
            "Episode: 1560, total reward: 200.0\n",
            "Episode: 1561, total reward: 200.0\n",
            "Episode: 1562, total reward: 200.0\n",
            "Episode: 1563, total reward: 200.0\n",
            "Episode: 1564, total reward: 200.0\n",
            "Episode: 1565, total reward: 200.0\n",
            "Episode: 1566, total reward: 200.0\n",
            "Episode: 1567, total reward: 200.0\n",
            "Episode: 1568, total reward: 200.0\n",
            "Episode: 1569, total reward: 200.0\n",
            "Episode: 1570, total reward: 200.0\n",
            "Episode: 1571, total reward: 200.0\n",
            "Episode: 1572, total reward: 200.0\n",
            "Episode: 1573, total reward: 200.0\n",
            "Episode: 1574, total reward: 200.0\n",
            "Episode: 1575, total reward: 200.0\n",
            "Episode: 1576, total reward: 200.0\n",
            "Episode: 1577, total reward: 200.0\n",
            "Episode: 1578, total reward: 200.0\n",
            "Episode: 1579, total reward: 200.0\n",
            "Episode: 1580, total reward: 200.0\n",
            "Episode: 1581, total reward: 200.0\n",
            "Episode: 1582, total reward: 200.0\n",
            "Episode: 1583, total reward: 200.0\n",
            "Episode: 1584, total reward: 200.0\n",
            "Episode: 1585, total reward: 200.0\n",
            "Episode: 1586, total reward: 200.0\n",
            "Episode: 1587, total reward: 200.0\n",
            "Episode: 1588, total reward: 200.0\n",
            "Episode: 1589, total reward: 200.0\n",
            "Episode: 1590, total reward: 200.0\n",
            "Episode: 1591, total reward: 200.0\n",
            "Episode: 1592, total reward: 200.0\n",
            "Episode: 1593, total reward: 200.0\n",
            "Episode: 1594, total reward: 200.0\n",
            "Episode: 1595, total reward: 200.0\n",
            "Episode: 1596, total reward: 200.0\n",
            "Episode: 1597, total reward: 200.0\n",
            "Episode: 1598, total reward: 200.0\n",
            "Episode: 1599, total reward: 200.0\n",
            "Episode: 1600, total reward: 200.0\n",
            "Episode: 1601, total reward: 200.0\n",
            "Episode: 1602, total reward: 200.0\n",
            "Episode: 1603, total reward: 200.0\n",
            "Episode: 1604, total reward: 200.0\n",
            "Episode: 1605, total reward: 200.0\n",
            "Episode: 1606, total reward: 200.0\n",
            "Episode: 1607, total reward: 200.0\n",
            "Episode: 1608, total reward: 200.0\n",
            "Episode: 1609, total reward: 200.0\n",
            "Episode: 1610, total reward: 200.0\n",
            "Episode: 1611, total reward: 200.0\n",
            "Episode: 1612, total reward: 200.0\n",
            "Episode: 1613, total reward: 200.0\n",
            "Episode: 1614, total reward: 200.0\n",
            "Episode: 1615, total reward: 200.0\n",
            "Episode: 1616, total reward: 200.0\n",
            "Episode: 1617, total reward: 200.0\n",
            "Episode: 1618, total reward: 200.0\n",
            "Episode: 1619, total reward: 200.0\n",
            "Episode: 1620, total reward: 200.0\n",
            "Episode: 1621, total reward: 200.0\n",
            "Episode: 1622, total reward: 200.0\n",
            "Episode: 1623, total reward: 200.0\n",
            "Episode: 1624, total reward: 200.0\n",
            "Episode: 1625, total reward: 200.0\n",
            "Episode: 1626, total reward: 200.0\n",
            "Episode: 1627, total reward: 200.0\n",
            "Episode: 1628, total reward: 200.0\n",
            "Episode: 1629, total reward: 200.0\n",
            "Episode: 1630, total reward: 200.0\n",
            "Episode: 1631, total reward: 200.0\n",
            "Episode: 1632, total reward: 200.0\n",
            "Episode: 1633, total reward: 200.0\n",
            "Episode: 1634, total reward: 200.0\n",
            "Episode: 1635, total reward: 200.0\n",
            "Episode: 1636, total reward: 200.0\n",
            "Episode: 1637, total reward: 200.0\n",
            "Episode: 1638, total reward: 200.0\n",
            "Episode: 1639, total reward: 200.0\n",
            "Episode: 1640, total reward: 200.0\n",
            "Episode: 1641, total reward: 200.0\n",
            "Episode: 1642, total reward: 200.0\n",
            "Episode: 1643, total reward: 200.0\n",
            "Episode: 1644, total reward: 200.0\n",
            "Episode: 1645, total reward: 200.0\n",
            "Episode: 1646, total reward: 200.0\n",
            "Episode: 1647, total reward: 200.0\n",
            "Episode: 1648, total reward: 200.0\n",
            "Episode: 1649, total reward: 200.0\n",
            "Episode: 1650, total reward: 200.0\n",
            "Episode: 1651, total reward: 200.0\n",
            "Episode: 1652, total reward: 200.0\n",
            "Episode: 1653, total reward: 200.0\n",
            "Episode: 1654, total reward: 200.0\n",
            "Episode: 1655, total reward: 200.0\n",
            "Episode: 1656, total reward: 200.0\n",
            "Episode: 1657, total reward: 200.0\n",
            "Episode: 1658, total reward: 200.0\n",
            "Episode: 1659, total reward: 200.0\n",
            "Episode: 1660, total reward: 200.0\n",
            "Episode: 1661, total reward: 200.0\n",
            "Episode: 1662, total reward: 200.0\n",
            "Episode: 1663, total reward: 200.0\n",
            "Episode: 1664, total reward: 200.0\n",
            "Episode: 1665, total reward: 200.0\n",
            "Episode: 1666, total reward: 200.0\n",
            "Episode: 1667, total reward: 200.0\n",
            "Episode: 1668, total reward: 200.0\n",
            "Episode: 1669, total reward: 200.0\n",
            "Episode: 1670, total reward: 200.0\n",
            "Episode: 1671, total reward: 200.0\n",
            "Episode: 1672, total reward: 200.0\n",
            "Episode: 1673, total reward: 200.0\n",
            "Episode: 1674, total reward: 200.0\n",
            "Episode: 1675, total reward: 200.0\n",
            "Episode: 1676, total reward: 200.0\n",
            "Episode: 1677, total reward: 200.0\n",
            "Episode: 1678, total reward: 200.0\n",
            "Episode: 1679, total reward: 200.0\n",
            "Episode: 1680, total reward: 200.0\n",
            "Episode: 1681, total reward: 200.0\n",
            "Episode: 1682, total reward: 200.0\n",
            "Episode: 1683, total reward: 200.0\n",
            "Episode: 1684, total reward: 200.0\n",
            "Episode: 1685, total reward: 200.0\n",
            "Episode: 1686, total reward: 200.0\n",
            "Episode: 1687, total reward: 200.0\n",
            "Episode: 1688, total reward: 200.0\n",
            "Episode: 1689, total reward: 200.0\n",
            "Episode: 1690, total reward: 200.0\n",
            "Episode: 1691, total reward: 200.0\n",
            "Episode: 1692, total reward: 200.0\n",
            "Episode: 1693, total reward: 200.0\n",
            "Episode: 1694, total reward: 200.0\n",
            "Episode: 1695, total reward: 200.0\n",
            "Episode: 1696, total reward: 200.0\n",
            "Episode: 1697, total reward: 200.0\n",
            "Episode: 1698, total reward: 200.0\n",
            "Episode: 1699, total reward: 200.0\n",
            "Episode: 1700, total reward: 200.0\n",
            "Episode: 1701, total reward: 200.0\n",
            "Episode: 1702, total reward: 200.0\n",
            "Episode: 1703, total reward: 200.0\n",
            "Episode: 1704, total reward: 200.0\n",
            "Episode: 1705, total reward: 200.0\n",
            "Episode: 1706, total reward: 200.0\n",
            "Episode: 1707, total reward: 200.0\n",
            "Episode: 1708, total reward: 200.0\n",
            "Episode: 1709, total reward: 200.0\n",
            "Episode: 1710, total reward: 200.0\n",
            "Episode: 1711, total reward: 200.0\n",
            "Episode: 1712, total reward: 200.0\n",
            "Episode: 1713, total reward: 200.0\n",
            "Episode: 1714, total reward: 200.0\n",
            "Episode: 1715, total reward: 200.0\n",
            "Episode: 1716, total reward: 200.0\n",
            "Episode: 1717, total reward: 200.0\n",
            "Episode: 1718, total reward: 200.0\n",
            "Episode: 1719, total reward: 200.0\n",
            "Episode: 1720, total reward: 200.0\n",
            "Episode: 1721, total reward: 200.0\n",
            "Episode: 1722, total reward: 200.0\n",
            "Episode: 1723, total reward: 200.0\n",
            "Episode: 1724, total reward: 200.0\n",
            "Episode: 1725, total reward: 200.0\n",
            "Episode: 1726, total reward: 200.0\n",
            "Episode: 1727, total reward: 200.0\n",
            "Episode: 1728, total reward: 200.0\n",
            "Episode: 1729, total reward: 200.0\n",
            "Episode: 1730, total reward: 200.0\n",
            "Episode: 1731, total reward: 200.0\n",
            "Episode: 1732, total reward: 200.0\n",
            "Episode: 1733, total reward: 200.0\n",
            "Episode: 1734, total reward: 200.0\n",
            "Episode: 1735, total reward: 200.0\n",
            "Episode: 1736, total reward: 200.0\n",
            "Episode: 1737, total reward: 200.0\n",
            "Episode: 1738, total reward: 200.0\n",
            "Episode: 1739, total reward: 200.0\n",
            "Episode: 1740, total reward: 200.0\n",
            "Episode: 1741, total reward: 200.0\n",
            "Episode: 1742, total reward: 200.0\n",
            "Episode: 1743, total reward: 200.0\n",
            "Episode: 1744, total reward: 200.0\n",
            "Episode: 1745, total reward: 200.0\n",
            "Episode: 1746, total reward: 200.0\n",
            "Episode: 1747, total reward: 200.0\n",
            "Episode: 1748, total reward: 200.0\n",
            "Episode: 1749, total reward: 200.0\n",
            "Episode: 1750, total reward: 200.0\n",
            "Episode: 1751, total reward: 200.0\n",
            "Episode: 1752, total reward: 200.0\n",
            "Episode: 1753, total reward: 200.0\n",
            "Episode: 1754, total reward: 200.0\n",
            "Episode: 1755, total reward: 200.0\n",
            "Episode: 1756, total reward: 200.0\n",
            "Episode: 1757, total reward: 200.0\n",
            "Episode: 1758, total reward: 200.0\n",
            "Episode: 1759, total reward: 200.0\n",
            "Episode: 1760, total reward: 200.0\n",
            "Episode: 1761, total reward: 200.0\n",
            "Episode: 1762, total reward: 200.0\n",
            "Episode: 1763, total reward: 200.0\n",
            "Episode: 1764, total reward: 200.0\n",
            "Episode: 1765, total reward: 200.0\n",
            "Episode: 1766, total reward: 200.0\n",
            "Episode: 1767, total reward: 200.0\n",
            "Episode: 1768, total reward: 200.0\n",
            "Episode: 1769, total reward: 200.0\n",
            "Episode: 1770, total reward: 200.0\n",
            "Episode: 1771, total reward: 200.0\n",
            "Episode: 1772, total reward: 200.0\n",
            "Episode: 1773, total reward: 200.0\n",
            "Episode: 1774, total reward: 200.0\n",
            "Episode: 1775, total reward: 200.0\n",
            "Episode: 1776, total reward: 200.0\n",
            "Episode: 1777, total reward: 200.0\n",
            "Episode: 1778, total reward: 200.0\n",
            "Episode: 1779, total reward: 200.0\n",
            "Episode: 1780, total reward: 200.0\n",
            "Episode: 1781, total reward: 200.0\n",
            "Episode: 1782, total reward: 200.0\n",
            "Episode: 1783, total reward: 200.0\n",
            "Episode: 1784, total reward: 200.0\n",
            "Episode: 1785, total reward: 200.0\n",
            "Episode: 1786, total reward: 200.0\n",
            "Episode: 1787, total reward: 200.0\n",
            "Episode: 1788, total reward: 200.0\n",
            "Episode: 1789, total reward: 200.0\n",
            "Episode: 1790, total reward: 200.0\n",
            "Episode: 1791, total reward: 200.0\n",
            "Episode: 1792, total reward: 200.0\n",
            "Episode: 1793, total reward: 200.0\n",
            "Episode: 1794, total reward: 200.0\n",
            "Episode: 1795, total reward: 200.0\n",
            "Episode: 1796, total reward: 200.0\n",
            "Episode: 1797, total reward: 200.0\n",
            "Episode: 1798, total reward: 200.0\n",
            "Episode: 1799, total reward: 200.0\n",
            "Episode: 1800, total reward: 200.0\n",
            "Episode: 1801, total reward: 200.0\n",
            "Episode: 1802, total reward: 200.0\n",
            "Episode: 1803, total reward: 200.0\n",
            "Episode: 1804, total reward: 200.0\n",
            "Episode: 1805, total reward: 200.0\n",
            "Episode: 1806, total reward: 200.0\n",
            "Episode: 1807, total reward: 200.0\n",
            "Episode: 1808, total reward: 200.0\n",
            "Episode: 1809, total reward: 200.0\n",
            "Episode: 1810, total reward: 200.0\n",
            "Episode: 1811, total reward: 200.0\n",
            "Episode: 1812, total reward: 200.0\n",
            "Episode: 1813, total reward: 200.0\n",
            "Episode: 1814, total reward: 200.0\n",
            "Episode: 1815, total reward: 200.0\n",
            "Episode: 1816, total reward: 200.0\n",
            "Episode: 1817, total reward: 200.0\n",
            "Episode: 1818, total reward: 200.0\n",
            "Episode: 1819, total reward: 200.0\n",
            "Episode: 1820, total reward: 200.0\n",
            "Episode: 1821, total reward: 200.0\n",
            "Episode: 1822, total reward: 200.0\n",
            "Episode: 1823, total reward: 200.0\n",
            "Episode: 1824, total reward: 200.0\n",
            "Episode: 1825, total reward: 200.0\n",
            "Episode: 1826, total reward: 200.0\n",
            "Episode: 1827, total reward: 200.0\n",
            "Episode: 1828, total reward: 200.0\n",
            "Episode: 1829, total reward: 200.0\n",
            "Episode: 1830, total reward: 200.0\n",
            "Episode: 1831, total reward: 200.0\n",
            "Episode: 1832, total reward: 200.0\n",
            "Episode: 1833, total reward: 200.0\n",
            "Episode: 1834, total reward: 200.0\n",
            "Episode: 1835, total reward: 200.0\n",
            "Episode: 1836, total reward: 200.0\n",
            "Episode: 1837, total reward: 200.0\n",
            "Episode: 1838, total reward: 200.0\n",
            "Episode: 1839, total reward: 200.0\n",
            "Episode: 1840, total reward: 200.0\n",
            "Episode: 1841, total reward: 200.0\n",
            "Episode: 1842, total reward: 200.0\n",
            "Episode: 1843, total reward: 200.0\n",
            "Episode: 1844, total reward: 200.0\n",
            "Episode: 1845, total reward: 200.0\n",
            "Episode: 1846, total reward: 200.0\n",
            "Episode: 1847, total reward: 200.0\n",
            "Episode: 1848, total reward: 200.0\n",
            "Episode: 1849, total reward: 200.0\n",
            "Episode: 1850, total reward: 200.0\n",
            "Episode: 1851, total reward: 200.0\n",
            "Episode: 1852, total reward: 200.0\n",
            "Episode: 1853, total reward: 200.0\n",
            "Episode: 1854, total reward: 200.0\n",
            "Episode: 1855, total reward: 200.0\n",
            "Episode: 1856, total reward: 200.0\n",
            "Episode: 1857, total reward: 200.0\n",
            "Episode: 1858, total reward: 200.0\n",
            "Episode: 1859, total reward: 200.0\n",
            "Episode: 1860, total reward: 200.0\n",
            "Episode: 1861, total reward: 200.0\n",
            "Episode: 1862, total reward: 200.0\n",
            "Episode: 1863, total reward: 200.0\n",
            "Episode: 1864, total reward: 200.0\n",
            "Episode: 1865, total reward: 200.0\n",
            "Episode: 1866, total reward: 200.0\n",
            "Episode: 1867, total reward: 200.0\n",
            "Episode: 1868, total reward: 200.0\n",
            "Episode: 1869, total reward: 200.0\n",
            "Episode: 1870, total reward: 200.0\n",
            "Episode: 1871, total reward: 200.0\n",
            "Episode: 1872, total reward: 200.0\n",
            "Episode: 1873, total reward: 200.0\n",
            "Episode: 1874, total reward: 200.0\n",
            "Episode: 1875, total reward: 200.0\n",
            "Episode: 1876, total reward: 200.0\n",
            "Episode: 1877, total reward: 200.0\n",
            "Episode: 1878, total reward: 200.0\n",
            "Episode: 1879, total reward: 200.0\n",
            "Episode: 1880, total reward: 200.0\n",
            "Episode: 1881, total reward: 200.0\n",
            "Episode: 1882, total reward: 200.0\n",
            "Episode: 1883, total reward: 200.0\n",
            "Episode: 1884, total reward: 200.0\n",
            "Episode: 1885, total reward: 200.0\n",
            "Episode: 1886, total reward: 200.0\n",
            "Episode: 1887, total reward: 200.0\n",
            "Episode: 1888, total reward: 200.0\n",
            "Episode: 1889, total reward: 200.0\n",
            "Episode: 1890, total reward: 200.0\n",
            "Episode: 1891, total reward: 200.0\n",
            "Episode: 1892, total reward: 200.0\n",
            "Episode: 1893, total reward: 200.0\n",
            "Episode: 1894, total reward: 200.0\n",
            "Episode: 1895, total reward: 200.0\n",
            "Episode: 1896, total reward: 200.0\n",
            "Episode: 1897, total reward: 200.0\n",
            "Episode: 1898, total reward: 200.0\n",
            "Episode: 1899, total reward: 200.0\n",
            "Episode: 1900, total reward: 200.0\n",
            "Episode: 1901, total reward: 200.0\n",
            "Episode: 1902, total reward: 200.0\n",
            "Episode: 1903, total reward: 200.0\n",
            "Episode: 1904, total reward: 200.0\n",
            "Episode: 1905, total reward: 200.0\n",
            "Episode: 1906, total reward: 200.0\n",
            "Episode: 1907, total reward: 200.0\n",
            "Episode: 1908, total reward: 200.0\n",
            "Episode: 1909, total reward: 200.0\n",
            "Episode: 1910, total reward: 200.0\n",
            "Episode: 1911, total reward: 200.0\n",
            "Episode: 1912, total reward: 200.0\n",
            "Episode: 1913, total reward: 200.0\n",
            "Episode: 1914, total reward: 200.0\n",
            "Episode: 1915, total reward: 200.0\n",
            "Episode: 1916, total reward: 200.0\n",
            "Episode: 1917, total reward: 200.0\n",
            "Episode: 1918, total reward: 200.0\n",
            "Episode: 1919, total reward: 200.0\n",
            "Episode: 1920, total reward: 200.0\n",
            "Episode: 1921, total reward: 200.0\n",
            "Episode: 1922, total reward: 200.0\n",
            "Episode: 1923, total reward: 200.0\n",
            "Episode: 1924, total reward: 200.0\n",
            "Episode: 1925, total reward: 200.0\n",
            "Episode: 1926, total reward: 200.0\n",
            "Episode: 1927, total reward: 200.0\n",
            "Episode: 1928, total reward: 200.0\n",
            "Episode: 1929, total reward: 200.0\n",
            "Episode: 1930, total reward: 200.0\n",
            "Episode: 1931, total reward: 200.0\n",
            "Episode: 1932, total reward: 200.0\n",
            "Episode: 1933, total reward: 200.0\n",
            "Episode: 1934, total reward: 200.0\n",
            "Episode: 1935, total reward: 200.0\n",
            "Episode: 1936, total reward: 200.0\n",
            "Episode: 1937, total reward: 200.0\n",
            "Episode: 1938, total reward: 200.0\n",
            "Episode: 1939, total reward: 200.0\n",
            "Episode: 1940, total reward: 200.0\n",
            "Episode: 1941, total reward: 200.0\n",
            "Episode: 1942, total reward: 200.0\n",
            "Episode: 1943, total reward: 200.0\n",
            "Episode: 1944, total reward: 200.0\n",
            "Episode: 1945, total reward: 200.0\n",
            "Episode: 1946, total reward: 200.0\n",
            "Episode: 1947, total reward: 200.0\n",
            "Episode: 1948, total reward: 200.0\n",
            "Episode: 1949, total reward: 200.0\n",
            "Episode: 1950, total reward: 200.0\n",
            "Episode: 1951, total reward: 200.0\n",
            "Episode: 1952, total reward: 200.0\n",
            "Episode: 1953, total reward: 200.0\n",
            "Episode: 1954, total reward: 200.0\n",
            "Episode: 1955, total reward: 200.0\n",
            "Episode: 1956, total reward: 200.0\n",
            "Episode: 1957, total reward: 200.0\n",
            "Episode: 1958, total reward: 200.0\n",
            "Episode: 1959, total reward: 200.0\n",
            "Episode: 1960, total reward: 200.0\n",
            "Episode: 1961, total reward: 200.0\n",
            "Episode: 1962, total reward: 200.0\n",
            "Episode: 1963, total reward: 200.0\n",
            "Episode: 1964, total reward: 200.0\n",
            "Episode: 1965, total reward: 200.0\n",
            "Episode: 1966, total reward: 200.0\n",
            "Episode: 1967, total reward: 200.0\n",
            "Episode: 1968, total reward: 200.0\n",
            "Episode: 1969, total reward: 200.0\n",
            "Episode: 1970, total reward: 200.0\n",
            "Episode: 1971, total reward: 200.0\n",
            "Episode: 1972, total reward: 200.0\n",
            "Episode: 1973, total reward: 200.0\n",
            "Episode: 1974, total reward: 200.0\n",
            "Episode: 1975, total reward: 200.0\n",
            "Episode: 1976, total reward: 200.0\n",
            "Episode: 1977, total reward: 200.0\n",
            "Episode: 1978, total reward: 200.0\n",
            "Episode: 1979, total reward: 200.0\n",
            "Episode: 1980, total reward: 200.0\n",
            "Episode: 1981, total reward: 200.0\n",
            "Episode: 1982, total reward: 200.0\n",
            "Episode: 1983, total reward: 200.0\n",
            "Episode: 1984, total reward: 200.0\n",
            "Episode: 1985, total reward: 200.0\n",
            "Episode: 1986, total reward: 200.0\n",
            "Episode: 1987, total reward: 200.0\n",
            "Episode: 1988, total reward: 200.0\n",
            "Episode: 1989, total reward: 200.0\n",
            "Episode: 1990, total reward: 200.0\n",
            "Episode: 1991, total reward: 200.0\n",
            "Episode: 1992, total reward: 200.0\n",
            "Episode: 1993, total reward: 200.0\n",
            "Episode: 1994, total reward: 200.0\n",
            "Episode: 1995, total reward: 200.0\n",
            "Episode: 1996, total reward: 200.0\n",
            "Episode: 1997, total reward: 200.0\n",
            "Episode: 1998, total reward: 200.0\n",
            "Episode: 1999, total reward: 200.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kHeLKx_ELG8u",
        "colab_type": "text"
      },
      "source": [
        "# Plot of Episode Reward over time"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "msY5lQBrLCsE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "687cdf97-f754-4e79-ad82-04ce7e4f46f1"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(total_reward_episode)\n",
        "plt.title('Episode reward over time')\n",
        "plt.xlabel('Episode')\n",
        "plt.ylabel('Total reward')\n",
        "plt.show()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BYwyddVQLqKQ",
        "colab_type": "text"
      },
      "source": [
        "# How it works\n",
        "REINFORCE relies heavily on Monte Carlo methods to generate a whole trajectory used to\n",
        "train the policy network. However, different actions may be taken in different episodes\n",
        "under the same stochastic policy. To reduce the variance for the sampled experience, we\n",
        "subtract the state-value from the return . The resulting advantage measures the reward\n",
        "relative to the average action, which will be used in the gradient update."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dNRJiMq9L0X7",
        "colab_type": "text"
      },
      "source": [
        "REINFORCE with a baseline algorithm does the following tasks:\n",
        "1. It runs an episodees the state, reward, and the log policy at each step.\n",
        "2. Once an episode finishes, it calculates the discounted cumulative reward at each\n",
        "step; it estimates the baseline values using the value network; it computes the\n",
        "advantage values by subtracting the baseline values from the returns.\n",
        "3. It computes policy gradients using the advantage values and log probabilities,\n",
        "and updates the policy and value networks. We also display the total reward for\n",
        "each episode.\n",
        "4.It runs n_episode episodes by repeating the aforementioned steps"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "79G8iA_OL9-h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}